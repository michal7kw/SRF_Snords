{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import shutil\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pysam\n",
    "\n",
    "# rpy2 imports\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri, r, Formula\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from rpy2.robjects.vectors import StrVector, DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the working directory\n",
    "# data_dir = \"/beegfs/scratch/ric.broccoli/ric.broccoli/PW_RNA_seq_deep/DexSeq_counts\"\n",
    "data_dir = \"/beegfs/scratch/ric.broccoli/kubacki.michal/SRF_Snords/Create_counts/output\"\n",
    "working_dir = \"/beegfs/scratch/ric.broccoli/kubacki.michal/SRF_Snords\"\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable automatic conversion between pandas and R dataframes\n",
    "pandas2ri.activate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary R packages\n",
    "dexseq = importr('DEXSeq')\n",
    "deseq2 = importr('DESeq2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_file_structures(file_list):\n",
    "    \"\"\"\n",
    "    Compare structures of multiple DEXSeq count files.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for file_path in file_list:\n",
    "        with open(file_path, 'r') as f:\n",
    "            n_lines = sum(1 for line in f)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        results.append({\n",
    "            'file': os.path.basename(file_path),\n",
    "            'n_lines': n_lines,\n",
    "            'size_mb': file_size / 1024 / 1024,\n",
    "            'avg_line_size': file_size / n_lines if n_lines > 0 else 0\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def parse_dexseq_id(feature_id):\n",
    "    \"\"\"Parse DEXSeq feature ID into gene ID and exon number.\"\"\"\n",
    "    if feature_id.startswith('*'):\n",
    "        return feature_id, None\n",
    "    try:\n",
    "        # Format is typically 'ENSG00000000003.14:\"001\"'\n",
    "        parts = feature_id.split(':')\n",
    "        gene_id = parts[0]\n",
    "        exon_num = parts[1].strip('\"')\n",
    "        return gene_id, exon_num\n",
    "    except:\n",
    "        return feature_id, None\n",
    "\n",
    "def examine_dexseq_file(file_path, n_head=5, n_random=5, n_tail=5):\n",
    "    \"\"\"\n",
    "    Examine a DEXSeq count file in detail with corrected parsing and error handling.\n",
    "    \"\"\"\n",
    "    print(f\"\\nExamining file: {os.path.basename(file_path)}\")\n",
    "    \n",
    "    # First, peek at the raw file contents\n",
    "    print(\"\\nFirst few lines (raw):\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < 5:\n",
    "                print(line.strip())\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # Read the file with correct column interpretation\n",
    "    try:\n",
    "        # Read file with custom separator and column names\n",
    "        df = pd.read_csv(file_path, sep='\\s+', header=None,\n",
    "                        names=['feature_id', 'count'])\n",
    "        \n",
    "        # Convert feature_id to string to handle potential float values\n",
    "        df['feature_id'] = df['feature_id'].astype(str)\n",
    "        \n",
    "        # Split feature_id into gene_id and exon_number\n",
    "        df['gene_id'], df['exon_number'] = zip(*df['feature_id'].apply(parse_dexseq_id))\n",
    "        \n",
    "        # File information\n",
    "        print(f\"\\nFile size: {os.path.getsize(file_path) / 1024:.2f} KB\")\n",
    "        print(f\"Number of lines: {len(df)}\")\n",
    "        \n",
    "        # Basic DataFrame information\n",
    "        print(\"\\nDataFrame Info:\")\n",
    "        print(df.info())\n",
    "        \n",
    "        # Print all column names\n",
    "        print(\"\\nColumn names:\")\n",
    "        print(df.columns.tolist())\n",
    "        \n",
    "        # Show first few lines\n",
    "        print(f\"\\nFirst {n_head} lines:\")\n",
    "        print(df.head(n_head))\n",
    "        \n",
    "        # Basic statistics for counts\n",
    "        print(\"\\nCount statistics:\")\n",
    "        print(df['count'].describe())\n",
    "        \n",
    "        # Additional information\n",
    "        print(\"\\nNumber of unique genes:\", df['gene_id'].nunique())\n",
    "        print(\"\\nTop 5 genes by total counts:\")\n",
    "        gene_counts = df.groupby('gene_id')['count'].sum().sort_values(ascending=False)\n",
    "        print(gene_counts.head())\n",
    "        \n",
    "        # Check for potential issues\n",
    "        print(\"\\nChecking for potential issues:\")\n",
    "        issues = []\n",
    "        if df['count'].isnull().any():\n",
    "            issues.append(\"- Contains missing values in counts\")\n",
    "        if (df['count'] < 0).any():\n",
    "            issues.append(\"- Contains negative counts\")\n",
    "        if not np.issubdtype(df['count'].dtype, np.number):\n",
    "            issues.append(\"- Counts are not numeric\")\n",
    "        if df['feature_id'].dtype != object:\n",
    "            issues.append(f\"- Feature ID column is not string type (current type: {df['feature_id'].dtype})\")\n",
    "            \n",
    "        # Check for special entries\n",
    "        special_entries = df[df['feature_id'].str.startswith('*')]\n",
    "        if not special_entries.empty:\n",
    "            print(\"\\nSpecial entries found:\")\n",
    "            print(special_entries)\n",
    "            \n",
    "        if len(issues) > 0:\n",
    "            print(\"\\nIssues found:\")\n",
    "            print(\"\\n\".join(issues))\n",
    "        else:\n",
    "            print(\"No major issues found\")\n",
    "            \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def examine_genes_of_interest(df, genes):\n",
    "    \"\"\"\n",
    "    Examine specific genes in the DEXSeq count data.\n",
    "    \"\"\"\n",
    "    print(\"\\nExamining genes of interest:\")\n",
    "    for gene in genes:\n",
    "        # Find all entries for this gene\n",
    "        gene_data = df[df['gene_id'].str.contains(gene, na=False)]\n",
    "        if not gene_data.empty:\n",
    "            print(f\"\\n{gene}:\")\n",
    "            print(f\"Number of exons: {len(gene_data)}\")\n",
    "            print(f\"Total counts: {gene_data['count'].sum()}\")\n",
    "            print(f\"Mean counts per exon: {gene_data['count'].mean():.2f}\")\n",
    "            print(\"\\nExon-level data:\")\n",
    "            print(gene_data.sort_values('exon_number'))\n",
    "            \n",
    "            # Create plot for this gene\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.bar(range(len(gene_data)), gene_data['count'])\n",
    "            plt.title(f'Exon counts for {gene}')\n",
    "            plt.xlabel('Exon number')\n",
    "            plt.ylabel('Counts')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'exon_counts_{gene}.pdf')\n",
    "            plt.close()\n",
    "\n",
    "def prepare_sample_table(sample_info):\n",
    "    \"\"\"\n",
    "    Prepare sample table in the format required by DEXSeq\n",
    "    \"\"\"\n",
    "    # Filter for EDO and ND1 samples (excluding PW1)\n",
    "    samples_subset = sample_info[sample_info['condition'].isin(['EDO', 'ND1'])].copy()\n",
    "    \n",
    "    # Reset index and ensure proper categorical variables\n",
    "    samples_subset = samples_subset.reset_index(drop=True)\n",
    "    samples_subset['condition'] = pd.Categorical(samples_subset['condition'])\n",
    "    samples_subset['replicate'] = pd.Categorical(samples_subset['replicate'])\n",
    "    \n",
    "    return samples_subset\n",
    "\n",
    "def create_dexseq_object(sample_table, dexseq):\n",
    "    \"\"\"\n",
    "    Create DEXSeqDataSet object from count files\n",
    "    \"\"\"\n",
    "    # Convert sample table to R dataframe\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        sample_table_r = ro.conversion.py2rpy(sample_table)\n",
    "    \n",
    "    # Create formula for the full model\n",
    "    formula = Formula('~ sample + exon + condition:exon')\n",
    "    \n",
    "    # Create formula for the reduced model\n",
    "    reduced_formula = Formula('~ sample + exon')\n",
    "    \n",
    "    # Create DEXSeqDataSet\n",
    "    dxd = dexseq.DEXSeqDataSetFromHTSeq(\n",
    "        countfiles=ro.StrVector(sample_table['count_file']),\n",
    "        sampleData=sample_table_r,\n",
    "        design=formula,\n",
    "        flattenedfile=None  # Set to None if you don't have a flattened annotation file\n",
    "    )\n",
    "    \n",
    "    return dxd\n",
    "\n",
    "def run_dexseq_analysis(dxd, dexseq):\n",
    "    \"\"\"\n",
    "    Run DEXSeq differential exon usage analysis\n",
    "    \"\"\"\n",
    "    # Normalize counts\n",
    "    dxd = dexseq.estimateSizeFactors(dxd)\n",
    "    \n",
    "    # Estimate dispersions\n",
    "    dxd = dexseq.estimateDispersions(dxd)\n",
    "    \n",
    "    # Test for differential exon usage\n",
    "    dxd = dexseq.testForDEU(dxd)\n",
    "    \n",
    "    # Estimate exon fold changes\n",
    "    dxd = dexseq.estimateExonFoldChanges(dxd)\n",
    "    \n",
    "    return dxd\n",
    "\n",
    "def extract_results(dxd, dexseq):\n",
    "    \"\"\"\n",
    "    Extract and format DEXSeq results\n",
    "    \"\"\"\n",
    "    # Get results table\n",
    "    res = dexseq.DEXSeqResults(dxd)\n",
    "    \n",
    "    # Convert R results to pandas DataFrame\n",
    "    with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "        results_df = ro.conversion.rpy2py(res)\n",
    "    \n",
    "    # Add adjusted p-value threshold\n",
    "    results_df['significant'] = results_df['padj'] < 0.05\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def filter_significant_results(results_df):\n",
    "    \"\"\"\n",
    "    Filter for significant differential exon usage\n",
    "    \"\"\"\n",
    "    # Filter for adjusted p-value < 0.05\n",
    "    significant_results = results_df[\n",
    "        (results_df['padj'] < 0.05) & \n",
    "        (results_df['log2fold_EDO_ND1'] != 0)\n",
    "    ].copy()\n",
    "    \n",
    "    # Sort by adjusted p-value\n",
    "    significant_results = significant_results.sort_values('padj')\n",
    "    \n",
    "    return significant_results\\\n",
    "\n",
    "def process_file(file):\n",
    "    \"\"\"\n",
    "    Process a DEXSeq count file and save the formatted version.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create absolute paths\n",
    "        output_dir = os.path.abspath(os.path.join(working_dir, 'DexSeq_counts'))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"Processing file: {file}\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "        \n",
    "        # Read the file\n",
    "        df = pd.read_csv(file, sep='\\t', header=None, names=['feature_id', 'count'])\n",
    "        \n",
    "        # Function to process feature_id\n",
    "        def process_feature_id(fid):\n",
    "            # Remove leading and trailing quotes\n",
    "            fid = str(fid).strip('\"')\n",
    "            # Check if it's a special entry\n",
    "            if fid.startswith('_'):\n",
    "                return pd.Series({'gene_id': fid, 'exon_id': None})\n",
    "            elif ':' in fid:\n",
    "                # Split into gene_id and exon_id\n",
    "                gene_id, exon_id = fid.split(':')\n",
    "                return pd.Series({'gene_id': gene_id, 'exon_id': exon_id.strip('\"')})\n",
    "            else:\n",
    "                # Handle any unexpected format\n",
    "                return pd.Series({'gene_id': fid, 'exon_id': None})\n",
    "\n",
    "        # Apply the function to the feature_id column\n",
    "        feature_split = df['feature_id'].apply(process_feature_id)\n",
    "        df['gene_id'] = feature_split['gene_id']\n",
    "        df['exon_id'] = feature_split['exon_id']\n",
    "\n",
    "        # Rearrange columns\n",
    "        df_formatted = df[['gene_id', 'exon_id', 'count']]\n",
    "\n",
    "        # Create output filename\n",
    "        output_file = os.path.join(output_dir, \n",
    "                                 os.path.basename(file).replace('.dexeq_counts', '.formatted.counts'))\n",
    "        \n",
    "        # Save the formatted file\n",
    "        df_formatted.to_csv(output_file, sep='\\t', header=False, index=False, na_rep='None')\n",
    "        \n",
    "        print(f\"Saved formatted file to: {output_file}\")\n",
    "        \n",
    "        # Verify file was created\n",
    "        if os.path.exists(output_file):\n",
    "            print(f\"File successfully created with size: {os.path.getsize(output_file)} bytes\")\n",
    "            return output_file\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Failed to create output file: {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEXSeq Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Feature Format File\n",
    "\"DATA/gencode.v31.basic.annotation.DEXSeq.gff\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **chr1** | **dexseq_prepare_annotation.py** | **aggregate_gene** | **11869** | **14409** | **.** | **+** | **.** | **gene_id \"ENSG00000223972.5\"**                                                                          |\n",
    "| -------- | -------------------------------- | ------------------ | --------- | --------- | ----- | ----- | ----- | -------------------------------------------------------------------------------------------------------- |\n",
    "| chr1     | dexseq_prepare_annotation.py     | exonic_part        | 11869     | 12009     | .     | +     | .     | gene_id \"ENSG00000223972.5\"; transcripts \"ENST00000456328.2\"; exonic_part_number \"001\"                   |\n",
    "| chr1     | dexseq_prepare_annotation.py     | exonic_part        | 12010     | 12057     | .     | +     | .     | gene_id \"ENSG00000223972.5\"; transcripts \"ENST00000450305.2+ENST00000456328.2\"; exonic_part_number \"002\" |\n",
    "| chr1     | dexseq_prepare_annotation.py     | exonic_part        | 12058     | 12178     | .     | +     | .     | gene_id \"ENSG00000223972.5\"; transcripts \"ENST00000456328.2\"; exonic_part_number \"003\"                   |\n",
    "| chr1     | dexseq_prepare_annotation.py     | exonic_part        | 12179     | 12227     | .     | +     | .     | gene_id \"ENSG00000223972.5\"; transcripts \"ENST00000450305.2+ENST00000456328.2\"; exonic_part_number \"004\" |\n",
    "| chr1     | dexseq_prepare_annotation.py     | exonic_part        | 12613     | 12697     | .     | +     | .     | gene_id \"ENSG00000223972.5\"; transcripts \"ENST00000450305.2+ENST00000456328.2\"; exonic_part_number \"005\" |\n",
    "| chr1     | dexseq_prepare_annotation.py     | exonic_part        | 12698     | 12721     | .     | +     | .     | gene_id \"ENSG00000223972.5\"; transcripts \"ENST00000456328.2\"; exonic_part_number \"006\"                   |\n",
    "| chr1     | dexseq_prepare_annotation.py     | exonic_part        | 12975     | 13052     | .     | +     | .     | gene_id \"ENSG00000223972.5\"; transcripts \"ENST00000450305.2\"; exonic_part_number \"007\"                   |\n",
    "| chr1     | dexseq_prepare_annotation.py     | exonic_part        | 13221     | 13374     | .     | +     | .     | gene_id \"ENSG00000223972.5\"; transcripts \"ENST00000450305.2+ENST00000456328.2\"; exonic_part_number \"008\" |\n",
    "| chr1     | dexseq_prepare_annotation.py     | exonic_part        | 13375     | 13452     | .     | +     | .     | gene_id \"ENSG00000223972.5\"; transcripts \"ENST00000456328.2\"; exonic_part_number \"009\"                   |\n",
    "| chr1     | dexseq_prepare_annotation.py     | exonic_part        | 13453     | 13670     | .     | +     | .     | gene_id \"ENSG00000223972.5\"; transcripts \"ENST00000450305.2+ENST00000456328.2\"; exonic_part_number \"010\" |\n",
    "| chr1     | dexseq_prepare_annotation.py     | exonic_part        | 13671     | 14409     | .     | +     | .     | gene_id \"ENSG00000223972.5\"; transcripts \"ENST00000456328.2\"; exonic_part_number \"011\"                   |\n",
    "| **chr1** | **dexseq_prepare_annotation.py** | **aggregate_gene** | **14404** | **29570** | **.** | **-** | **.** | **gene_id \"ENSG00000227232.5\"**                                                                          |\n",
    "| chr1     | dexseq_prepare_annotation.py     | exonic_part        | 14404     | 14501     | .     | -     | .     | gene_id \"ENSG00000227232.5\"; transcripts \"ENST00000488147.1\"; exonic_part_number \"001\"                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a list of your count files\n",
    "# count_files = glob.glob(os.path.join(data_dir, '*.dexeq_counts'))  # Remove 'DexSeq_counts/' from the path\n",
    "\n",
    "# print(f\"Found {len(count_files)} count files to process\")\n",
    "# print(\"\\nInput files found:\")\n",
    "# for f in count_files:  # Print all files\n",
    "#     print(f\"  {f} (exists: {os.path.exists(f)})\")\n",
    "\n",
    "# # Process files in parallel with better error handling\n",
    "# processed_files = []\n",
    "# with ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:\n",
    "#     future_to_file = {executor.submit(process_file, file): file for file in count_files}\n",
    "#     for future in as_completed(future_to_file):\n",
    "#         file = future_to_file[future]\n",
    "#         try:\n",
    "#             result = future.result()\n",
    "#             if result:\n",
    "#                 processed_files.append(result)\n",
    "#                 print(f\"Successfully processed: {os.path.basename(file)}\")\n",
    "#             else:\n",
    "#                 print(f\"Failed to process: {os.path.basename(file)}\")\n",
    "#         except Exception as exc:\n",
    "#             print(f'{file} generated an exception: {exc}')\n",
    "\n",
    "# print(f\"\\nSuccessfully processed {len(processed_files)} files\")\n",
    "\n",
    "# # Verify the results\n",
    "# if processed_files:\n",
    "#     print(\"\\nFirst 10 lines of first processed file:\")\n",
    "#     with open(processed_files[0], 'r') as f:\n",
    "#         for _ in range(10):\n",
    "#             line = f.readline().strip()\n",
    "#             print(line)\n",
    "# else:\n",
    "#     print(\"No files were successfully processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DEXSeq count files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for DEXSeq count files\n",
    "dexseq_dir = os.path.join(working_dir, \"DexSeq_counts\")\n",
    "count_files = [f for f in os.listdir(dexseq_dir) if f.endswith('.formatted.counts')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sample information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample information\n",
    "sample_info = pd.DataFrame({\n",
    "    'sample': [f.replace('.formatted.counts', '') for f in count_files],\n",
    "    'condition': ['EDO' if f.startswith('EDO') else 'ND1' if f.startswith('ND1') else 'PW1' if f.startswith('PW1') else 'Unknown' for f in count_files],\n",
    "    'replicate': [f.split('.')[0][-1] for f in count_files],\n",
    "    'count_file': [os.path.join(dexseq_dir, f) for f in count_files]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for EDO and ND1 samples (excluding PW1)\n",
    "edo_nd1_samples = sample_info[sample_info['condition'].isin(['EDO', 'ND1'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure proper formatting for DEXSeq\n",
    "edo_nd1_samples = edo_nd1_samples.reset_index(drop=True)\n",
    "edo_nd1_samples['condition'] = pd.Categorical(edo_nd1_samples['condition'])\n",
    "edo_nd1_samples['replicate'] = pd.Categorical(edo_nd1_samples['replicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edo_nd1_samples.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of count files\n",
    "count_files = edo_nd1_samples['count_file'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine structure of files\n",
    "print(\"Comparing file structures:\")\n",
    "structure_comparison = compare_file_structures(count_files)\n",
    "print(structure_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine first file in detail\n",
    "first_file = count_files[0]\n",
    "df = examine_dexseq_file(first_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot overall count distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "counts = df['count'][df['count'] > 0]\n",
    "plt.hist(counts, bins=50, color='skyblue', edgecolor='black', range=(0, counts.quantile(0.99)))\n",
    "plt.title('Count Distribution (> 0)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Counts', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "plt.xlim(0, counts.quantile(0.99))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print outliers\n",
    "outliers = df[df['count'] > counts.quantile(0.99)]\n",
    "print(\"\\nOutliers (counts above 99th percentile):\")\n",
    "# print(outliers[['feature_id', 'count', 'gene_id']].to_string(index=False))\n",
    "display(outliers.head())\n",
    "\n",
    "# Plot outliers distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(131)\n",
    "outlier_counts = outliers['count']\n",
    "plt.hist(outlier_counts, bins=50, color='salmon', edgecolor='black')\n",
    "plt.title('Outlier Count Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Counts', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.hist(np.log10(outlier_counts), bins=50, color='lightgreen', edgecolor='black')\n",
    "plt.title('Log10 Outlier Count Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Log10(Counts)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.boxplot(outlier_counts)\n",
    "plt.title('Outlier Count Boxplot', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Counts', fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEXSeq Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gff_features():\n",
    "    \"\"\"\n",
    "    Extract features from GFF file\n",
    "    \"\"\"\n",
    "    gff_features = set()\n",
    "    with open(\"DATA/gencode.v31.basic.annotation.gff\", 'r') as f:\n",
    "        for line in f:\n",
    "            if 'exonic_part' in line:\n",
    "                fields = line.strip().split('\\t')\n",
    "                attrs = dict(item.strip().split(' ', 1) for item in fields[8].strip().split(';'))\n",
    "                gene_id = attrs['gene_id'].strip('\"')\n",
    "                exon_num = attrs['exonic_part_number'].strip('\"')\n",
    "                feature_id = f\"{gene_id}:E{exon_num}\"\n",
    "                gff_features.add(feature_id)\n",
    "    return gff_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dexseq_dataset(sample_info, processed_files, dexseq):\n",
    "    \"\"\"\n",
    "    Create DEXSeqDataSet with proper formatting\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare sample data\n",
    "        sample_data = pd.DataFrame({\n",
    "            'sample': sample_info['sample'],\n",
    "            'condition': sample_info['condition']\n",
    "        })\n",
    "        \n",
    "        print(\"\\nSample data:\")\n",
    "        print(sample_data)\n",
    "        \n",
    "        # Convert to R objects\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "            sample_data_r = ro.conversion.py2rpy(sample_data)\n",
    "        \n",
    "        # Create DEXSeqDataSet\n",
    "        dxd = dexseq.DEXSeqDataSetFromHTSeq(\n",
    "            countfiles=ro.StrVector(processed_files),\n",
    "            sampleData=sample_data_r,\n",
    "            design=Formula('~ sample + exon + condition:exon'),\n",
    "            flattenedfile=\"DATA/gencode.v31.basic.annotation.gff\"\n",
    "        )\n",
    "        \n",
    "        return dxd\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DEXSeq dataset: {str(e)}\")\n",
    "        # Add more debugging information\n",
    "        print(\"\\nChecking processed files:\")\n",
    "        for f in processed_files:\n",
    "            print(f\"\\nFile: {os.path.basename(f)}\")\n",
    "            with open(f, 'r') as file:\n",
    "                print(file.read(500))\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_count_file(file_path, gff_features):\n",
    "    \"\"\"\n",
    "    Process DEXSeq count file to match the GFF feature format\n",
    "    \"\"\"\n",
    "    output_dir = os.path.dirname(file_path)\n",
    "    basename = os.path.basename(file_path)\n",
    "    output_path = os.path.join(output_dir, f\"processed_{basename}\")\n",
    "    \n",
    "    try:\n",
    "        # Create ordered dictionary of gff features\n",
    "        gff_feature_dict = {feature: True for feature in sorted(gff_features)}\n",
    "        \n",
    "        # Read and process file\n",
    "        count_dict = {}\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if not line.startswith('_'):  # Skip special entries\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) == 2:\n",
    "                        feature_id, count = parts\n",
    "                        # Remove quotes and split feature ID\n",
    "                        feature_id = feature_id.strip('\"')\n",
    "                        if '\":\"' in feature_id:\n",
    "                            gene_id, exon_num = feature_id.split('\":\"')\n",
    "                            exon_num = exon_num.strip('\"')\n",
    "                            # Format to match GFF style\n",
    "                            feature_id = f\"{gene_id}:E{exon_num}\"\n",
    "                            \n",
    "                            # Only keep features that exist in GFF\n",
    "                            if feature_id in gff_feature_dict:\n",
    "                                count_dict[feature_id] = int(count)\n",
    "        \n",
    "        # Create output with all GFF features (using 0 for missing counts)\n",
    "        with open(output_path, 'w') as f:\n",
    "            for feature in gff_feature_dict:\n",
    "                count = count_dict.get(feature, 0)\n",
    "                f.write(f\"{feature}\\t{count}\\n\")\n",
    "        \n",
    "        print(f\"\\nProcessed {basename}\")\n",
    "        print(f\"Features written: {len(gff_feature_dict)}\")\n",
    "        \n",
    "        # Verify file contents\n",
    "        with open(output_path, 'r') as f:\n",
    "            first_lines = [next(f) for _ in range(5)]\n",
    "        print(\"First few lines:\")\n",
    "        for line in first_lines:\n",
    "            print(line.strip())\n",
    "        \n",
    "        return output_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {basename}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def verify_processed_file(file_path, gff_features):\n",
    "    \"\"\"\n",
    "    Verify that a processed file matches GFF features exactly\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_features = set()\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                feature_id = line.strip().split('\\t')[0]\n",
    "                file_features.add(feature_id)\n",
    "        \n",
    "        missing_features = gff_features - file_features\n",
    "        extra_features = file_features - gff_features\n",
    "        \n",
    "        print(f\"\\nVerification of {os.path.basename(file_path)}:\")\n",
    "        print(f\"Total features in file: {len(file_features)}\")\n",
    "        print(f\"Missing features: {len(missing_features)}\")\n",
    "        print(f\"Extra features: {len(extra_features)}\")\n",
    "        \n",
    "        return len(missing_features) == 0 and len(extra_features) == 0\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying {file_path}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    # Set up paths\n",
    "    # data_dir = \"/beegfs/scratch/ric.broccoli/ric.broccoli/PW_RNA_seq_deep/DexSeq_counts\"\n",
    "    data_dir = \"/beegfs/scratch/ric.broccoli/kubacki.michal/SRF_Snords/Create_counts/output\"    \n",
    "    working_dir = \"/beegfs/scratch/ric.broccoli/kubacki.michal/SRF_Snords\"\n",
    "    os.chdir(working_dir)\n",
    "    \n",
    "    # Get GFF features first\n",
    "    print(\"Loading GFF features...\")\n",
    "    gff_features = get_gff_features()\n",
    "    print(f\"Found {len(gff_features)} features in GFF\")\n",
    "    print(\"Sample GFF features:\")\n",
    "    for feature in sorted(list(gff_features))[:5]:\n",
    "        print(feature)\n",
    "    \n",
    "    # Get count files\n",
    "    count_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) \n",
    "                  if f.endswith('.dexeq_counts')]\n",
    "    print(f\"\\nFound {len(count_files)} count files\")\n",
    "    \n",
    "    # Process count files\n",
    "    processed_files = []\n",
    "    for file in count_files:\n",
    "        processed_file = process_count_file(file, gff_features)\n",
    "        if processed_file and verify_processed_file(processed_file, gff_features):\n",
    "            processed_files.append(processed_file)\n",
    "    \n",
    "    if not processed_files:\n",
    "        print(\"No files were processed successfully\")\n",
    "        return\n",
    "    \n",
    "    # Create sample info for EDO and ND1 samples only\n",
    "    edo_nd1_samples = pd.DataFrame([\n",
    "        {'sample': os.path.basename(f).replace('.dexeq_counts', '').replace('processed_', ''),\n",
    "         'condition': 'EDO' if 'EDO' in f else 'ND1'}\n",
    "        for f in processed_files\n",
    "        if 'EDO' in f or 'ND1' in f\n",
    "    ])\n",
    "    \n",
    "    if len(edo_nd1_samples) == 0:\n",
    "        print(\"No EDO or ND1 samples found\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nFinal sample information:\")\n",
    "    print(edo_nd1_samples)\n",
    "    \n",
    "    # Import DEXSeq and create dataset\n",
    "    dexseq = importr('DEXSeq')\n",
    "    try:\n",
    "        dxd = create_dexseq_dataset(edo_nd1_samples, \n",
    "                                  [f for f in processed_files if any(s in f for s in edo_nd1_samples['sample'])],\n",
    "                                  dexseq)\n",
    "        print(\"Successfully created DEXSeq dataset!\")\n",
    "        return dxd\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFailed to create DEXSeq dataset: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Enable automatic conversion between pandas and R dataframes\n",
    "    pandas2ri.activate()\n",
    "    \n",
    "    # Run the analysis\n",
    "    dxd = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check original count file\n",
    "count_file = count_files[0]\n",
    "print(f\"\\nOriginal count file ({os.path.basename(count_file)}):\")\n",
    "with open(count_file, 'r') as f:\n",
    "    print(f.read(500))\n",
    "\n",
    "# Check processed file\n",
    "processed_file = processed_files[0]\n",
    "print(f\"\\nProcessed file ({os.path.basename(processed_file)}):\")\n",
    "with open(processed_file, 'r') as f:\n",
    "    print(f.read(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dexseq_dataset(sample_info, formatted_files, dexseq):\n",
    "    \"\"\"\n",
    "    Create DEXSeqDataSet with proper sample ordering and include 'sample' as a column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the sample_info DataFrame as-is since 'sample' is now a column\n",
    "        # sample_data = sample_info[['sample', 'condition']]\n",
    "        sample_data = sample_info[['sample', 'condition']].copy()\n",
    "\n",
    "        # Convert columns to string type\n",
    "        sample_data['sample'] = sample_data['sample'].astype(str)\n",
    "        sample_data['condition'] = sample_data['condition'].astype(str)\n",
    "\n",
    "        # Ensure formatted_files correspond to the samples\n",
    "        ordered_files = sample_info['count_file'].tolist()\n",
    "\n",
    "        print(\"\\nSample data for DEXSeq:\")\n",
    "        print(sample_data)\n",
    "\n",
    "        print(\"\\nOrdered count files:\")\n",
    "        for sample, file in zip(sample_data['sample'], ordered_files):\n",
    "            print(f\"{sample}: {file}\")\n",
    "\n",
    "        # Convert sample_data to an R DataFrame\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "            sample_data_r = ro.conversion.py2rpy(sample_data)\n",
    "\n",
    "        # Convert count files list to an R vector\n",
    "        count_files_vector = ro.StrVector(ordered_files)\n",
    "\n",
    "        # After converting to R objects\n",
    "        print(\"R sample_data:\")\n",
    "        print(r.str(sample_data_r))\n",
    "        print(\"\\nR count_files_vector:\")\n",
    "        print(r.str(count_files_vector))\n",
    "\n",
    "        # Also, let's check the length of these objects\n",
    "        print(\"\\nLength of sample_data_r:\", r.length(sample_data_r))\n",
    "        print(\"Length of count_files_vector:\", r.length(count_files_vector))\n",
    "\n",
    "        gff_file = \"DATA/gencode.v31.basic.annotation.DEXSeq.gff\"\n",
    "        if not os.path.exists(gff_file):\n",
    "            print(f\"GFF file not found: {gff_file}\")\n",
    "        else:\n",
    "            print(f\"GFF file exists: {gff_file}\")\n",
    "            # Print first few lines of the GFF file\n",
    "            with open(gff_file, 'r') as f:\n",
    "                print(f.read(500))\n",
    "\n",
    "        # Create DEXSeqDataSet\n",
    "        dxd = dexseq.DEXSeqDataSetFromHTSeq(\n",
    "            countfiles=count_files_vector,\n",
    "            sampleData=sample_data_r,\n",
    "            # design=Formula('~ sample + exon + condition:exon'),\n",
    "            design=Formula('~ condition'),\n",
    "            flattenedfile=gff_file        )\n",
    "\n",
    "        return dxd\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DEXSeqDataSet: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head DexSeq_counts/EDO_1.formatted.counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail DexSeq_counts/EDO_1.formatted.counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "grep -P '\\texonic_part\\t' DATA/gencode.v31.basic.annotation.DEXSeq.gff | \\\n",
    "awk 'BEGIN{FS=\"\\t\"} { \n",
    "    split($9, a, \";\"); \n",
    "    for (i in a) { \n",
    "        if (a[i] ~ /gene_id/) { \n",
    "            gsub(/gene_id| |\"/, \"\", a[i]); \n",
    "            gene_id = a[i];\n",
    "        } else if (a[i] ~ /exonic_part_number/) {\n",
    "            gsub(/exonic_part_number| |\"/, \"\", a[i]); \n",
    "            exon_id = a[i];\n",
    "        }\n",
    "    } \n",
    "    print gene_id\"\\t\"exon_id\n",
    "}' > exonic_parts_from_gff.txt\n",
    "\n",
    "# Display the first few lines of the output file\n",
    "head exonic_parts_from_gff.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "grep -P '\\texonic_part\\t' DATA/gencode.v31.basic.annotation.gff | \\\n",
    "awk 'BEGIN{FS=\"\\t\"} { \n",
    "    split($9, a, \";\"); \n",
    "    for (i in a) { \n",
    "        if (a[i] ~ /gene_id/) { \n",
    "            gsub(/gene_id| |\"/, \"\", a[i]); \n",
    "            gene_id = a[i];\n",
    "        } else if (a[i] ~ /exonic_part_number/) {\n",
    "            gsub(/exonic_part_number| |\"/, \"\", a[i]); \n",
    "            exon_id = a[i];\n",
    "        }\n",
    "    } \n",
    "    print gene_id\"\\t\"exon_id\n",
    "}' > exonic_parts_from_gff.txt\n",
    "\n",
    "# Display the first few lines of the output file\n",
    "head exonic_parts_from_gff.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep ENSG00000000003.14 exonic_parts_from_gff.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep ENSG00000223972.5 /beegfs/scratch/ric.broccoli/kubacki.michal/SRF_Snords/DexSeq_counts/EDO_1.formatted.counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l /beegfs/scratch/ric.broccoli/kubacki.michal/SRF_Snords/DexSeq_counts/*.formatted.counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 /beegfs/scratch/ric.broccoli/kubacki.michal/SRF_Snords/gencode.v31.basic.annotation.DEXSeq.gff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in count_files:\n",
    "    if not os.path.exists(file):\n",
    "        print(f\"File not found: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(count_files[0], 'r') as f:\n",
    "    print(f.read(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_gff_file(gff_path):\n",
    "    \"\"\"\n",
    "    Analyze the structure of a DEXSeq GFF file and compare with count files\n",
    "    \"\"\"\n",
    "    print(\"=== GFF File Analysis ===\")\n",
    "    \n",
    "    # Read first few lines of GFF\n",
    "    print(\"\\nFirst few lines of GFF file:\")\n",
    "    with open(gff_path, 'r') as f:\n",
    "        header_lines = []\n",
    "        feature_lines = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i < 10:  # Store first 10 lines for analysis\n",
    "                if not line.startswith('#'):\n",
    "                    feature_lines.append(line.strip())\n",
    "                else:\n",
    "                    header_lines.append(line.strip())\n",
    "    \n",
    "    print(\"\\nHeaders:\")\n",
    "    for line in header_lines:\n",
    "        print(line)\n",
    "        \n",
    "    print(\"\\nFirst few features:\")\n",
    "    for line in feature_lines[:3]:\n",
    "        print(line)\n",
    "    \n",
    "    # Analyze feature structure\n",
    "    features = []\n",
    "    gene_exon_map = {}\n",
    "    with open(gff_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if 'exonic_part' in line:\n",
    "                fields = line.strip().split('\\t')\n",
    "                if len(fields) >= 9:\n",
    "                    attrs = dict(item.strip().split(' ', 1) for item in fields[8].strip().split(';'))\n",
    "                    gene_id = attrs.get('gene_id', '').strip('\"')\n",
    "                    exon_num = attrs.get('exonic_part_number', '').strip('\"')\n",
    "                    feature_id = f\"{gene_id}:E{exon_num}\"\n",
    "                    features.append({\n",
    "                        'feature_id': feature_id,\n",
    "                        'gene_id': gene_id,\n",
    "                        'exon_num': exon_num,\n",
    "                        'type': fields[2],\n",
    "                        'attributes': attrs\n",
    "                    })\n",
    "                    gene_exon_map.setdefault(gene_id, set()).add(exon_num)\n",
    "    \n",
    "    df = pd.DataFrame(features)\n",
    "    \n",
    "    print(\"\\nFeature Statistics:\")\n",
    "    print(f\"Total features: {len(features)}\")\n",
    "    print(f\"Unique genes: {len(gene_exon_map)}\")\n",
    "    print(\"\\nSample of feature structure:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    return df, gene_exon_map\n",
    "\n",
    "def analyze_count_file(count_path, gene_exon_map):\n",
    "    \"\"\"\n",
    "    Analyze a count file and compare with GFF structure\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Count File Analysis: {count_path} ===\")\n",
    "    \n",
    "    missing_features = []\n",
    "    with open(count_path, 'r') as f:\n",
    "        count_features = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i < 5:  # Print first few lines\n",
    "                print(f\"Line {i+1}: {line.strip()}\")\n",
    "            \n",
    "            if not line.startswith('_'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) >= 2:\n",
    "                    feature_id = parts[0]\n",
    "                    # Check if this feature exists in GFF\n",
    "                    gene_id = feature_id.split(':')[0] if ':' in feature_id else ''\n",
    "                    exon_num = feature_id.split(':E')[1] if ':E' in feature_id else ''\n",
    "                    \n",
    "                    if gene_id and exon_num:\n",
    "                        if gene_id not in gene_exon_map or exon_num not in gene_exon_map[gene_id]:\n",
    "                            missing_features.append(feature_id)\n",
    "                    \n",
    "                    count_features.append(feature_id)\n",
    "    \n",
    "    print(f\"\\nTotal features in count file: {len(count_features)}\")\n",
    "    print(f\"Missing features in GFF: {len(missing_features)}\")\n",
    "    if missing_features:\n",
    "        print(\"\\nSample of missing features:\")\n",
    "        for feature in missing_features[:5]:\n",
    "            print(feature)\n",
    "            \n",
    "    return count_features, missing_features\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    gff_path = \"DATA/gencode.v31.basic.annotation.DEXSeq.gff\"\n",
    "    count_path = \"DexSeq_counts/EDO_1.formatted.counts\"  # Replace with your count file\n",
    "    \n",
    "    gff_df, gene_exon_map = analyze_gff_file(gff_path)\n",
    "    count_features, missing = analyze_count_file(count_path, gene_exon_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before creating DEXSeq dataset\n",
    "print(\"\\nVerifying GFF features...\")\n",
    "with open(\"DATA/gencode.v31.basic.annotation.DEXSeq.gff\", 'r') as f:\n",
    "    gff_features = set()\n",
    "    for line in f:\n",
    "        if 'exonic_part' in line:\n",
    "            fields = line.strip().split('\\t')\n",
    "            attrs = dict(item.strip().split(' ', 1) for item in fields[8].strip().split(';'))\n",
    "            gene_id = attrs['gene_id'].strip('\"')\n",
    "            exon_num = attrs['exonic_part_number'].strip('\"')\n",
    "            feature_id = f\"{gene_id}:E{exon_num}\"\n",
    "            gff_features.add(feature_id)\n",
    "\n",
    "print(f\"Number of features in GFF: {len(gff_features)}\")\n",
    "print(\"\\nVerifying count files...\")\n",
    "for pf in processed_files[:1]:  # Check first file\n",
    "    with open(pf, 'r') as f:\n",
    "        count_features = set(line.split('\\t')[0] for line in f)\n",
    "    print(f\"Number of features in count file: {len(count_features)}\")\n",
    "    print(f\"Features matching GFF: {len(count_features.intersection(gff_features))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_count_file_with_mapping(file_path, ordered_features, gff_gene_mapping):\n",
    "    \"\"\"\n",
    "    Process count file to match DEXSeq format with ordered GFF features\n",
    "    \"\"\"\n",
    "    output_dir = os.path.dirname(file_path)\n",
    "    basename = os.path.basename(file_path)\n",
    "    output_path = os.path.join(output_dir, f\"fixed_{basename}\")\n",
    "    \n",
    "    try:\n",
    "        # Create a mapping of gene_id + exon_num to GFF feature_id\n",
    "        feature_mapping = {}\n",
    "        for feature in ordered_features:\n",
    "            gene_id, exon_part = feature.split(':')\n",
    "            base_gene_id = gene_id.split('.')[0]\n",
    "            exon_num = exon_part[1:]  # Remove 'E' prefix\n",
    "            key = (base_gene_id, exon_num)\n",
    "            feature_mapping[key] = feature\n",
    "        \n",
    "        # Read counts into dictionary\n",
    "        counts = {}\n",
    "        unmapped_genes = set()\n",
    "        print(f\"\\nProcessing {basename}\")\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if not line.startswith('_'):\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) == 3:\n",
    "                        gene_id, exon_num, count = parts\n",
    "                        # Clean up IDs\n",
    "                        base_gene_id = gene_id.strip('\"').split('.')[0]\n",
    "                        exon_num = exon_num.strip('\"').zfill(3)\n",
    "                        \n",
    "                        # Try to find matching feature\n",
    "                        key = (base_gene_id, exon_num)\n",
    "                        if key in feature_mapping:\n",
    "                            feature_id = feature_mapping[key]\n",
    "                            counts[feature_id] = int(count)\n",
    "                            if i < 5:  # Debug first few lines\n",
    "                                print(f\"Line {i+1}: {gene_id} -> {feature_id}\")\n",
    "                                print(f\"Count: {count}\")\n",
    "                        else:\n",
    "                            unmapped_genes.add(base_gene_id)\n",
    "                            if i < 5:\n",
    "                                print(f\"Warning: No mapping for {gene_id}:E{exon_num}\")\n",
    "\n",
    "        # Report unmapped genes\n",
    "        if unmapped_genes:\n",
    "            print(f\"\\nWarning: {len(unmapped_genes)} genes not found in GFF\")\n",
    "            print(\"First few unmapped genes:\", list(unmapped_genes)[:5])\n",
    "\n",
    "        # Create output with all GFF features in the correct order\n",
    "        features_with_counts = 0\n",
    "        with open(output_path, 'w') as f:\n",
    "            for feature_id in ordered_features:\n",
    "                count = counts.get(feature_id, 0)\n",
    "                if count > 0:\n",
    "                    features_with_counts += 1\n",
    "                f.write(f\"{feature_id}\\t{count}\\n\")\n",
    "        \n",
    "        print(f\"\\nFeatures with non-zero counts: {features_with_counts}\")\n",
    "        \n",
    "        # Verify output format\n",
    "        print(\"\\nVerifying output file (first 5 lines):\")\n",
    "        with open(output_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i < 5:\n",
    "                    print(line.strip())\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {basename}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_ordered_gff_features(gff_path):\n",
    "    \"\"\"\n",
    "    Get list of features and create mapping from GFF file\n",
    "    \"\"\"\n",
    "    ordered_features = []\n",
    "    gene_versions = {}\n",
    "    \n",
    "    try:\n",
    "        print(\"\\nExamining GFF file structure:\")\n",
    "        with open(gff_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if 'exonic_part' in line:\n",
    "                    fields = line.strip().split('\\t')\n",
    "                    attrs = dict(item.strip().split(' ', 1) for item in fields[8].strip().split(';'))\n",
    "                    gene_id = attrs['gene_id'].strip('\"')\n",
    "                    base_gene_id = gene_id.split('.')[0]\n",
    "                    gene_versions[base_gene_id] = gene_id\n",
    "                    \n",
    "                    exon_num = attrs['exonic_part_number'].strip('\"')\n",
    "                    feature_id = f\"{gene_id}:E{exon_num}\"\n",
    "                    ordered_features.append(feature_id)\n",
    "                    \n",
    "                    # Show example entries\n",
    "                    if i < 5:\n",
    "                        print(f\"\\nGFF entry {i+1}:\")\n",
    "                        print(f\"Original gene_id: {attrs['gene_id']}\")\n",
    "                        print(f\"Cleaned gene_id: {gene_id}\")\n",
    "                        print(f\"Feature ID: {feature_id}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading GFF file: {str(e)}\")\n",
    "        raise\n",
    "        \n",
    "    return ordered_features, gene_versions\n",
    "\n",
    "# Main processing\n",
    "print(\"Loading GFF features...\")\n",
    "gff_path = \"DATA/gencode.v31.basic.annotation.gff\"\n",
    "ordered_features, gene_versions = get_ordered_gff_features(gff_path)\n",
    "print(f\"\\nFound {len(ordered_features)} features in GFF file\")\n",
    "print(f\"Found {len(gene_versions)} unique genes in GFF file\")\n",
    "print(\"\\nSample features:\")\n",
    "for feature in ordered_features[:5]:\n",
    "    print(feature)\n",
    "\n",
    "# Process all files\n",
    "processed_files = []\n",
    "for file in count_files:\n",
    "    processed_file = process_count_file_with_mapping(file, ordered_features, gene_versions)\n",
    "    if processed_file:\n",
    "        processed_files.append(processed_file)\n",
    "\n",
    "if len(processed_files) == len(edo_nd1_samples):\n",
    "    print(\"\\nAll files processed successfully\")\n",
    "    \n",
    "    # Verify processed files\n",
    "    if verify_count_files(processed_files, ordered_features):\n",
    "        print(\"\\nAll files verified successfully\")\n",
    "        edo_nd1_samples['count_file'] = processed_files\n",
    "        \n",
    "        try:\n",
    "            dxd = create_dexseq_dataset(edo_nd1_samples, processed_files, dexseq)\n",
    "            print(\"Successfully created DEXSeq dataset!\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError creating DEXSeq dataset: {str(e)}\")\n",
    "            print(\"\\nDetailed error information:\")\n",
    "            print(\"Sample data:\")\n",
    "            print(edo_nd1_samples)\n",
    "            print(\"\\nFirst few lines of first count file:\")\n",
    "            with open(processed_files[0], 'r') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if i < 5:\n",
    "                        print(line.strip())\n",
    "    else:\n",
    "        print(\"\\nSome files failed verification\")\n",
    "else:\n",
    "    print(f\"\\nWarning: Only processed {len(processed_files)} out of {len(edo_nd1_samples)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_count_file_with_mapping(file_path, ordered_features):\n",
    "#     \"\"\"\n",
    "#     Process count file to match DEXSeq format with ordered GFF features\n",
    "#     \"\"\"\n",
    "#     output_dir = os.path.dirname(file_path)\n",
    "#     basename = os.path.basename(file_path)\n",
    "#     output_path = os.path.join(output_dir, f\"fixed_{basename}\")\n",
    "    \n",
    "#     try:\n",
    "#         # Create a mapping of gene_id + exon_num to GFF feature_id\n",
    "#         gff_mapping = {}\n",
    "#         for feature in ordered_features:\n",
    "#             gene_id, exon_part = feature.split(':')\n",
    "#             exon_num = exon_part[1:]  # Remove 'E' prefix\n",
    "#             key = (gene_id, exon_num)\n",
    "#             gff_mapping[key] = feature\n",
    "        \n",
    "#         # Read counts into dictionary\n",
    "#         counts = {}\n",
    "#         print(f\"\\nProcessing {basename}\")\n",
    "#         with open(file_path, 'r') as f:\n",
    "#             for i, line in enumerate(f):\n",
    "#                 if not line.startswith('_'):\n",
    "#                     parts = line.strip().split('\\t')\n",
    "#                     if len(parts) == 3:\n",
    "#                         gene_id, exon_num, count = parts\n",
    "#                         # Clean up IDs\n",
    "#                         gene_id = gene_id.strip('\"')\n",
    "#                         exon_num = exon_num.strip('\"').zfill(3)\n",
    "                        \n",
    "#                         # Try to find matching feature\n",
    "#                         key = (gene_id, exon_num)\n",
    "#                         if key in gff_mapping:\n",
    "#                             feature_id = gff_mapping[key]\n",
    "#                             counts[feature_id] = int(count)\n",
    "                        \n",
    "#                         # Debug first few lines\n",
    "#                         if i < 5:\n",
    "#                             print(f\"Line {i+1}: {gene_id}\\t{exon_num}\\t{count}\")\n",
    "#                             print(f\"Mapped to feature: {gff_mapping.get(key, 'NOT FOUND')}\")\n",
    "\n",
    "#         # Create output with all GFF features in the correct order\n",
    "#         with open(output_path, 'w') as f:\n",
    "#             for feature_id in ordered_features:\n",
    "#                 count = counts.get(feature_id, 0)\n",
    "#                 f.write(f\"{feature_id}\\t{count}\\n\")\n",
    "        \n",
    "#         # Verify non-zero counts\n",
    "#         total_counts = sum(1 for count in counts.values() if count > 0)\n",
    "#         print(f\"\\nFeatures with non-zero counts: {total_counts}\")\n",
    "        \n",
    "#         return output_path\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {basename}: {str(e)}\")\n",
    "#         return None\n",
    "\n",
    "# Add this function to examine the original count files\n",
    "def examine_original_count_file(file_path):\n",
    "    \"\"\"\n",
    "    Examine the structure of original count files\n",
    "    \"\"\"\n",
    "    print(f\"\\nExamining {os.path.basename(file_path)}\")\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            print(\"First 5 lines:\")\n",
    "            for i, line in enumerate(f):\n",
    "                if i < 5:\n",
    "                    print(line.strip())\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) == 3:\n",
    "                        gene_id, exon_num, count = parts\n",
    "                        print(f\"Parsed: gene_id='{gene_id}' exon_num='{exon_num}' count='{count}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error examining file: {str(e)}\")\n",
    "\n",
    "# Before processing, examine original files\n",
    "print(\"Examining original count files...\")\n",
    "for file in count_files[:1]:  # Check first file\n",
    "    examine_original_count_file(file)\n",
    "\n",
    "def create_dexseq_dataset(sample_info, count_files, dexseq):\n",
    "    \"\"\"\n",
    "    Create DEXSeqDataSet with strict format checking and alignment\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create sample data with explicit index matching count files\n",
    "        sample_data = pd.DataFrame({\n",
    "            'sample': sample_info['sample'],\n",
    "            'condition': sample_info['condition']\n",
    "        }).reset_index(drop=True)\n",
    "        \n",
    "        # Ensure count files match sample order\n",
    "        print(\"\\nVerifying alignment:\")\n",
    "        print(\"Sample data:\")\n",
    "        print(sample_data)\n",
    "        print(\"\\nCount files:\")\n",
    "        for cf in count_files:\n",
    "            print(os.path.basename(cf))\n",
    "            \n",
    "        # Sort count files to match sample order\n",
    "        sorted_count_files = []\n",
    "        for sample in sample_data['sample']:\n",
    "            matching_file = [f for f in count_files if sample in os.path.basename(f)]\n",
    "            if matching_file:\n",
    "                sorted_count_files.append(matching_file[0])\n",
    "            else:\n",
    "                raise ValueError(f\"No matching count file found for sample {sample}\")\n",
    "        \n",
    "        print(\"\\nAligned count files:\")\n",
    "        for sample, cf in zip(sample_data['sample'], sorted_count_files):\n",
    "            print(f\"{sample}: {os.path.basename(cf)}\")\n",
    "        \n",
    "        # Convert to R objects\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "            sample_data_r = ro.conversion.py2rpy(sample_data)\n",
    "        \n",
    "        # Create DEXSeqDataSet with verbose error checking\n",
    "        print(\"\\nCreating DEXSeqDataSet with:\")\n",
    "        print(f\"Number of samples: {len(sample_data)}\")\n",
    "        print(f\"Number of count files: {len(sorted_count_files)}\")\n",
    "        \n",
    "        # Use absolute paths\n",
    "        gff_path = os.path.abspath(\"DATA/gencode.v31.basic.annotation.DEXSeq.gff\")\n",
    "        sorted_count_files = [os.path.abspath(f) for f in sorted_count_files]\n",
    "        \n",
    "        print(\"\\nUsing GFF file:\", gff_path)\n",
    "        print(\"First count file:\", sorted_count_files[0])\n",
    "        \n",
    "        dxd = dexseq.DEXSeqDataSetFromHTSeq(\n",
    "            countfiles=ro.StrVector(sorted_count_files),\n",
    "            sampleData=sample_data_r,\n",
    "            design=Formula('~ sample + exon + condition:exon'),\n",
    "            flattenedfile=gff_path\n",
    "        )\n",
    "        \n",
    "        return dxd\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DEXSeqDataSet: {str(e)}\")\n",
    "        # Additional error information\n",
    "        print(\"\\nDetailed error check:\")\n",
    "        print(f\"Sample data shape: {sample_data.shape}\")\n",
    "        print(\"Sample data columns:\", sample_data.columns.tolist())\n",
    "        print(\"\\nCount file contents verification:\")\n",
    "        for cf in sorted_count_files[:1]:  # Check first file\n",
    "            print(f\"\\nChecking {os.path.basename(cf)}\")\n",
    "            try:\n",
    "                with open(cf, 'r') as f:\n",
    "                    first_lines = [next(f) for _ in range(5)]\n",
    "                print(\"First 5 lines:\")\n",
    "                for line in first_lines:\n",
    "                    print(line.strip())\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_ordered_gff_features(gff_path):\n",
    "    \"\"\"\n",
    "    Get list of features from GFF file in correct order\n",
    "    \"\"\"\n",
    "    ordered_features = []\n",
    "    try:\n",
    "        with open(gff_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if 'exonic_part' in line:\n",
    "                    fields = line.strip().split('\\t')\n",
    "                    attrs = dict(item.strip().split(' ', 1) for item in fields[8].strip().split(';'))\n",
    "                    gene_id = attrs['gene_id'].strip('\"')\n",
    "                    exon_num = attrs['exonic_part_number'].strip('\"')\n",
    "                    feature_id = f\"{gene_id}:E{exon_num}\"\n",
    "                    ordered_features.append(feature_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading GFF file: {str(e)}\")\n",
    "        raise\n",
    "        \n",
    "    return ordered_features\n",
    "\n",
    "def verify_count_files(count_files, ordered_features):\n",
    "    \"\"\"\n",
    "    Verify all count files have correct format and features\n",
    "    \"\"\"\n",
    "    print(\"\\nVerifying count files...\")\n",
    "    all_valid = True\n",
    "    \n",
    "    for cf in count_files:\n",
    "        print(f\"\\nChecking {os.path.basename(cf)}\")\n",
    "        try:\n",
    "            with open(cf, 'r') as f:\n",
    "                file_features = [line.split('\\t')[0] for line in f]\n",
    "            \n",
    "            if file_features == ordered_features:\n",
    "                print(\"✓ Features match GFF order\")\n",
    "            else:\n",
    "                print(\"✗ Features do not match GFF order\")\n",
    "                all_valid = False\n",
    "                \n",
    "                # Show first mismatch\n",
    "                for i, (expected, actual) in enumerate(zip(ordered_features, file_features)):\n",
    "                    if expected != actual:\n",
    "                        print(f\"First mismatch at position {i}:\")\n",
    "                        print(f\"  Expected: {expected}\")\n",
    "                        print(f\"  Found: {actual}\")\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            print(f\"Error verifying file: {str(e)}\")\n",
    "            all_valid = False\n",
    "    \n",
    "    return all_valid\n",
    "\n",
    "# Main processing\n",
    "print(\"Loading GFF features...\")\n",
    "gff_path = \"DATA/gencode.v31.basic.annotation.DEXSeq.gff\"\n",
    "ordered_features = get_ordered_gff_features(gff_path)\n",
    "print(f\"Found {len(ordered_features)} features in GFF file\")\n",
    "print(\"Sample features:\")\n",
    "for feature in ordered_features[:5]:\n",
    "    print(feature)\n",
    "\n",
    "# Process all files\n",
    "# processed_files = []\n",
    "# for file in count_files:\n",
    "#     processed_file = process_count_file_with_mapping(file, ordered_features)\n",
    "#     if processed_file:\n",
    "#         processed_files.append(processed_file)\n",
    "\n",
    "if len(processed_files) == len(edo_nd1_samples):\n",
    "    print(\"\\nAll files processed successfully\")\n",
    "    \n",
    "    # Verify processed files\n",
    "    if verify_count_files(processed_files, ordered_features):\n",
    "        print(\"\\nAll files verified successfully\")\n",
    "        edo_nd1_samples['count_file'] = processed_files\n",
    "        \n",
    "        try:\n",
    "            dxd = create_dexseq_dataset(edo_nd1_samples, processed_files, dexseq)\n",
    "            print(\"Successfully created DEXSeq dataset!\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError creating DEXSeq dataset: {str(e)}\")\n",
    "            print(\"\\nChecking file formats:\")\n",
    "            for pf in processed_files:\n",
    "                print(f\"\\nFile: {os.path.basename(pf)}\")\n",
    "                with open(pf, 'r') as f:\n",
    "                    for i, line in enumerate(f):\n",
    "                        if i < 5:  # Print first 5 lines\n",
    "                            print(line.strip())\n",
    "    else:\n",
    "        print(\"\\nSome files failed verification\")\n",
    "else:\n",
    "    print(f\"\\nWarning: Only processed {len(processed_files)} out of {len(edo_nd1_samples)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_gff_line(line):\n",
    "    \"\"\"Parse a GFF line and return structured data\"\"\"\n",
    "    fields = line.strip().split('\\t')\n",
    "    if len(fields) < 9:\n",
    "        return None\n",
    "        \n",
    "    attrs = dict(item.strip().split(' ', 1) for item in fields[8].strip().split(';'))\n",
    "    return {\n",
    "        'chromosome': fields[0],\n",
    "        'source': fields[1],\n",
    "        'feature': fields[2],\n",
    "        'start': fields[3],\n",
    "        'end': fields[4],\n",
    "        'strand': fields[6],\n",
    "        'attributes': attrs,\n",
    "        'raw_line': line.strip()\n",
    "    }\n",
    "\n",
    "def analyze_gff_format(gff_path):\n",
    "    \"\"\"Detailed analysis of GFF file format\"\"\"\n",
    "    print(f\"\\nAnalyzing GFF file: {gff_path}\")\n",
    "    \n",
    "    features = []\n",
    "    attributes_seen = set()\n",
    "    feature_types = set()\n",
    "    \n",
    "    with open(gff_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if line.startswith('#') or not line.strip():\n",
    "                continue\n",
    "                \n",
    "            parsed = parse_gff_line(line)\n",
    "            if parsed:\n",
    "                feature_types.add(parsed['feature'])\n",
    "                attributes_seen.update(parsed['attributes'].keys())\n",
    "                \n",
    "                if parsed['feature'] == 'exonic_part':\n",
    "                    gene_id = parsed['attributes'].get('gene_id', '').strip('\"')\n",
    "                    exon_num = parsed['attributes'].get('exonic_part_number', '').strip('\"')\n",
    "                    if gene_id and exon_num:\n",
    "                        features.append({\n",
    "                            'feature_id': f\"{gene_id}:E{exon_num}\",\n",
    "                            'gene_id': gene_id,\n",
    "                            'exon_num': exon_num,\n",
    "                            'line_num': i + 1\n",
    "                        })\n",
    "    \n",
    "    print(\"\\nGFF Analysis:\")\n",
    "    print(f\"Total features: {len(features)}\")\n",
    "    print(f\"Feature types: {feature_types}\")\n",
    "    print(f\"Attributes found: {attributes_seen}\")\n",
    "    print(\"\\nFirst few features:\")\n",
    "    for f in features[:5]:\n",
    "        print(f\"Line {f['line_num']}: {f['feature_id']}\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "def analyze_count_file(count_path):\n",
    "    \"\"\"Detailed analysis of count file format\"\"\"\n",
    "    print(f\"\\nAnalyzing count file: {count_path}\")\n",
    "    \n",
    "    features = []\n",
    "    with open(count_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if line.startswith('_') or not line.strip():\n",
    "                continue\n",
    "                \n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                feature_id, count = parts\n",
    "                features.append({\n",
    "                    'feature_id': feature_id,\n",
    "                    'count': count,\n",
    "                    'line_num': i + 1\n",
    "                })\n",
    "    \n",
    "    print(f\"\\nCount file analysis:\")\n",
    "    print(f\"Total features: {len(features)}\")\n",
    "    print(\"\\nFirst few features:\")\n",
    "    for f in features[:5]:\n",
    "        print(f\"Line {f['line_num']}: {f['feature_id']} -> {f['count']}\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "def compare_formats(gff_features, count_features):\n",
    "    \"\"\"Compare GFF and count file features\"\"\"\n",
    "    print(\"\\nComparing formats:\")\n",
    "    \n",
    "    gff_ids = set(f['feature_id'] for f in gff_features)\n",
    "    count_ids = set(f['feature_id'] for f in count_features)\n",
    "    \n",
    "    missing_in_counts = gff_ids - count_ids\n",
    "    extra_in_counts = count_ids - gff_ids\n",
    "    \n",
    "    print(f\"\\nFeatures in GFF: {len(gff_ids)}\")\n",
    "    print(f\"Features in counts: {len(count_ids)}\")\n",
    "    print(f\"Missing in counts: {len(missing_in_counts)}\")\n",
    "    print(f\"Extra in counts: {len(extra_in_counts)}\")\n",
    "    \n",
    "    if missing_in_counts:\n",
    "        print(\"\\nSample missing features:\")\n",
    "        for feat in list(missing_in_counts)[:5]:\n",
    "            print(f\"  {feat}\")\n",
    "    \n",
    "    if extra_in_counts:\n",
    "        print(\"\\nSample extra features:\")\n",
    "        for feat in list(extra_in_counts)[:5]:\n",
    "            print(f\"  {feat}\")\n",
    "    \n",
    "    # Check feature ID format\n",
    "    print(\"\\nFeature ID format analysis:\")\n",
    "    gff_sample = list(gff_ids)[0]\n",
    "    count_sample = list(count_ids)[0]\n",
    "    print(f\"GFF format:   {gff_sample}\")\n",
    "    print(f\"Count format: {count_sample}\")\n",
    "    \n",
    "    return len(missing_in_counts) == 0 and len(extra_in_counts) == 0\n",
    "\n",
    "def fix_count_file(count_path, gff_features, output_dir=None):\n",
    "    \"\"\"Fix count file based on GFF features\"\"\"\n",
    "    if output_dir is None:\n",
    "        output_dir = Path(count_path).parent / \"fixed\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"fixed_{Path(count_path).name}\")\n",
    "    \n",
    "    # Create ordered dict of GFF features\n",
    "    gff_ordered = {f['feature_id']: i for i, f in enumerate(gff_features)}\n",
    "    \n",
    "    # Read current counts\n",
    "    counts = {}\n",
    "    with open(count_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if not line.startswith('_'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    feature_id, count = parts\n",
    "                    counts[feature_id] = int(count)\n",
    "    \n",
    "    # Write fixed file with exact GFF order\n",
    "    with open(output_path, 'w') as f:\n",
    "        for feature in gff_features:\n",
    "            feature_id = feature['feature_id']\n",
    "            count = counts.get(feature_id, 0)\n",
    "            f.write(f\"{feature_id}\\t{count}\\n\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    gff_path = \"DATA/gencode.v31.basic.annotation.DEXSeq.gff\"\n",
    "    count_path = \"DexSeq_counts/fixed_EDO_1.formatted.counts\"  # Update with your count file\n",
    "    \n",
    "    # Analyze files\n",
    "    gff_features = analyze_gff_format(gff_path)\n",
    "    count_features = analyze_count_file(count_path)\n",
    "    \n",
    "    # Compare formats\n",
    "    formats_match = compare_formats(gff_features, count_features)\n",
    "    \n",
    "    if not formats_match:\n",
    "        print(\"\\nFixing count file...\")\n",
    "        fixed_path = fix_count_file(count_path, gff_features)\n",
    "        print(f\"Fixed file written to: {fixed_path}\")\n",
    "        \n",
    "        # Verify fix\n",
    "        fixed_features = analyze_count_file(fixed_path)\n",
    "        fixed_match = compare_formats(gff_features, fixed_features)\n",
    "        if fixed_match:\n",
    "            print(\"\\n✓ File successfully fixed\")\n",
    "        else:\n",
    "            print(\"\\n✗ Issues remain after fixing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_count_file_with_mapping(file_path, gff_features):\n",
    "    output_dir = os.path.dirname(file_path)\n",
    "    basename = os.path.basename(file_path)\n",
    "    output_path = os.path.join(output_dir, f\"fixed_{basename}\")\n",
    "    \n",
    "    # Create ordered list of features from GFF\n",
    "    ordered_features = []\n",
    "    with open(\"DATA/gencode.v31.basic.annotation.DEXSeq.gff\", 'r') as f:\n",
    "        for line in f:\n",
    "            if 'exonic_part' in line:\n",
    "                fields = line.strip().split('\\t')\n",
    "                attrs = dict(item.strip().split(' ', 1) for item in fields[8].strip().split(';'))\n",
    "                gene_id = attrs['gene_id'].strip('\"')\n",
    "                exon_num = attrs['exonic_part_number'].strip('\"')\n",
    "                feature_id = f\"{gene_id}:E{exon_num}\"\n",
    "                ordered_features.append(feature_id)\n",
    "    \n",
    "    # Read counts into dictionary\n",
    "    counts = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if not line.startswith('_'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) == 3:\n",
    "                    gene_id, exon_num, count = parts\n",
    "                    gene_id = gene_id.strip('\"')\n",
    "                    exon_num = exon_num.strip('\"').zfill(3)\n",
    "                    feature_id = f\"{gene_id}:E{exon_num}\"\n",
    "                    counts[feature_id] = int(count)\n",
    "    \n",
    "    # Create output with all GFF features in order\n",
    "    with open(output_path, 'w') as f:\n",
    "        for feature_id in ordered_features:\n",
    "            count = counts.get(feature_id, 0)  # Use 0 for missing features\n",
    "            f.write(f\"{feature_id}\\t{count}\\n\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def create_dexseq_dataset(sample_info, count_files, dexseq):\n",
    "    \"\"\"\n",
    "    Create DEXSeqDataSet with strict format checking\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create sample data\n",
    "        sample_data = pd.DataFrame({\n",
    "            'sample': sample_info['sample'],\n",
    "            'condition': sample_info['condition']\n",
    "        })\n",
    "        \n",
    "        print(\"\\nSample data:\")\n",
    "        print(sample_data)\n",
    "        \n",
    "        # Convert to R objects\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "            sample_data_r = ro.conversion.py2rpy(sample_data)\n",
    "        \n",
    "        # Create DEXSeqDataSet\n",
    "        dxd = dexseq.DEXSeqDataSetFromHTSeq(\n",
    "            countfiles=ro.StrVector(count_files),\n",
    "            sampleData=sample_data_r,\n",
    "            # design=Formula('~ condition'),  # Simplified design\n",
    "            design=Formula('~ sample + exon + condition:exon'),\n",
    "            flattenedfile=\"DATA/gencode.v31.basic.annotation.DEXSeq.gff\"\n",
    "        )\n",
    "        \n",
    "        return dxd\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DEXSeqDataSet: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# First, load and parse GFF features\n",
    "print(\"Loading GFF features...\")\n",
    "gff_features = set()\n",
    "with open(\"DATA/gencode.v31.basic.annotation.DEXSeq.gff\", 'r') as f:\n",
    "    for line in f:\n",
    "        if 'exonic_part' in line:\n",
    "            fields = line.strip().split('\\t')\n",
    "            attrs = dict(item.strip().split(' ', 1) for item in fields[8].strip().split(';'))\n",
    "            gene_id = attrs['gene_id'].strip('\"')\n",
    "            exon_num = attrs['exonic_part_number'].strip('\"')\n",
    "            feature_id = f\"{gene_id}:E{exon_num}\"\n",
    "            gff_features.add(feature_id)\n",
    "\n",
    "print(f\"Found {len(gff_features)} features in GFF file\")\n",
    "print(\"Sample features:\")\n",
    "for feature in list(gff_features)[:5]:\n",
    "    print(feature)\n",
    "\n",
    "# Process all files\n",
    "processed_files = []\n",
    "for file in count_files:\n",
    "    processed_file = process_count_file_with_mapping(file, gff_features)\n",
    "    if processed_file:\n",
    "        processed_files.append(processed_file)\n",
    "\n",
    "if len(processed_files) == len(edo_nd1_samples):\n",
    "    print(\"\\nAll files processed successfully\")\n",
    "    edo_nd1_samples['count_file'] = processed_files\n",
    "    \n",
    "    try:\n",
    "        dxd = create_dexseq_dataset(edo_nd1_samples, processed_files, dexseq)\n",
    "        print(\"Successfully created DEXSeq dataset!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError creating DEXSeq dataset: {str(e)}\")\n",
    "        # Print detailed debugging information\n",
    "        print(\"\\nChecking file formats:\")\n",
    "        for pf in processed_files:\n",
    "            print(f\"\\nFile: {os.path.basename(pf)}\")\n",
    "            with open(pf, 'r') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if i < 5:  # Print first 5 lines\n",
    "                        print(line.strip())\n",
    "else:\n",
    "    print(f\"\\nWarning: Only processed {len(processed_files)} out of {len(edo_nd1_samples)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_count_file_with_mapping(file_path, ordered_features):\n",
    "    \"\"\"\n",
    "    Process count file to match DEXSeq format with ordered GFF features.\n",
    "    DEXSeq expects the format: gene_id exon_number count\n",
    "    \"\"\"\n",
    "    output_dir = os.path.dirname(file_path)\n",
    "    basename = os.path.basename(file_path)\n",
    "    output_path = os.path.join(output_dir, f\"fixed_{basename}\")\n",
    "    \n",
    "    try:\n",
    "        # Read counts into dictionary\n",
    "        counts = {}\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if not line.startswith('_'):\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) == 2:  # Current format: GENE:EXON count\n",
    "                        feature_id, count = parts\n",
    "                        # Split feature ID into gene and exon\n",
    "                        gene_id, exon_part = feature_id.split(':')\n",
    "                        exon_num = exon_part[1:]  # Remove 'E' prefix\n",
    "                        counts[feature_id] = int(count)\n",
    "\n",
    "        # Create output with DEXSeq expected format\n",
    "        with open(output_path, 'w') as f:\n",
    "            for feature_id in ordered_features:\n",
    "                # Split feature ID into components\n",
    "                gene_id, exon_part = feature_id.split(':')\n",
    "                exon_num = exon_part[1:]  # Remove 'E' prefix\n",
    "                count = counts.get(feature_id, 0)\n",
    "                # Write in DEXSeq format: gene_id exon_number count\n",
    "                f.write(f'{gene_id}\\t{exon_num}\\t{count}\\n')\n",
    "        \n",
    "        # Verify the output\n",
    "        print(f\"\\nProcessing {basename}\")\n",
    "        feature_count = len(ordered_features)\n",
    "        print(f\"Written {feature_count} features\")\n",
    "        print(\"\\nSample of processed data:\")\n",
    "        with open(output_path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i < 5:\n",
    "                    print(line.strip())\n",
    "        \n",
    "        return output_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {basename}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_ordered_gff_features(gff_path):\n",
    "    \"\"\"\n",
    "    Get list of features from GFF file in correct order\n",
    "    \"\"\"\n",
    "    ordered_features = []\n",
    "    try:\n",
    "        with open(gff_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if 'exonic_part' in line:\n",
    "                    fields = line.strip().split('\\t')\n",
    "                    attrs = dict(item.strip().split(' ', 1) for item in fields[8].strip().split(';'))\n",
    "                    gene_id = attrs['gene_id'].strip('\"')\n",
    "                    exon_num = attrs['exonic_part_number'].strip('\"')\n",
    "                    feature_id = f\"{gene_id}:E{exon_num}\"\n",
    "                    ordered_features.append(feature_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading GFF file: {str(e)}\")\n",
    "        raise\n",
    "        \n",
    "    return ordered_features\n",
    "\n",
    "def verify_count_files(count_files):\n",
    "    \"\"\"\n",
    "    Verify all count files have correct DEXSeq format\n",
    "    \"\"\"\n",
    "    print(\"\\nVerifying count files...\")\n",
    "    all_valid = True\n",
    "    \n",
    "    for cf in count_files:\n",
    "        print(f\"\\nChecking {os.path.basename(cf)}\")\n",
    "        try:\n",
    "            with open(cf, 'r') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if i < 5:  # Print first few lines for verification\n",
    "                        print(line.strip())\n",
    "                    # Check format: should be gene_id exon_number count\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) != 3:\n",
    "                        print(f\"✗ Invalid format in line {i+1}: expected 3 columns, got {len(parts)}\")\n",
    "                        all_valid = False\n",
    "                        break\n",
    "                    gene_id, exon_num, count = parts\n",
    "                    try:\n",
    "                        int(exon_num)\n",
    "                        int(count)\n",
    "                    except ValueError:\n",
    "                        print(f\"✗ Invalid numeric format in line {i+1}\")\n",
    "                        all_valid = False\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            print(f\"Error verifying file: {str(e)}\")\n",
    "            all_valid = False\n",
    "    \n",
    "    return all_valid\n",
    "\n",
    "# Main processing\n",
    "print(\"Loading GFF features...\")\n",
    "gff_path = \"DATA/gencode.v31.basic.annotation.DEXSeq.gff\"\n",
    "ordered_features = get_ordered_gff_features(gff_path)\n",
    "print(f\"Found {len(ordered_features)} features in GFF file\")\n",
    "print(\"Sample features:\")\n",
    "for feature in ordered_features[:5]:\n",
    "    print(feature)\n",
    "\n",
    "# Process all files\n",
    "processed_files = []\n",
    "for file in count_files:\n",
    "    processed_file = process_count_file_with_mapping(file, ordered_features)\n",
    "    if processed_file:\n",
    "        processed_files.append(processed_file)\n",
    "\n",
    "if len(processed_files) == len(edo_nd1_samples):\n",
    "    print(\"\\nAll files processed successfully\")\n",
    "    \n",
    "    # Verify processed files\n",
    "    if verify_count_files(processed_files):\n",
    "        print(\"\\nAll files verified successfully\")\n",
    "        edo_nd1_samples['count_file'] = processed_files\n",
    "        \n",
    "        try:\n",
    "            dxd = create_dexseq_dataset(edo_nd1_samples, processed_files, dexseq)\n",
    "            print(\"Successfully created DEXSeq dataset!\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError creating DEXSeq dataset: {str(e)}\")\n",
    "            print(\"\\nChecking file formats:\")\n",
    "            for pf in processed_files:\n",
    "                print(f\"\\nFile: {os.path.basename(pf)}\")\n",
    "                with open(pf, 'r') as f:\n",
    "                    for i, line in enumerate(f):\n",
    "                        if i < 5:  # Print first 5 lines\n",
    "                            print(line.strip())\n",
    "    else:\n",
    "        print(\"\\nSome files failed verification\")\n",
    "else:\n",
    "    print(f\"\\nWarning: Only processed {len(processed_files)} out of {len(edo_nd1_samples)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dexseq_dataset(sample_info, count_files, dexseq):\n",
    "    \"\"\"\n",
    "    Create DEXSeqDataSet with strict format checking and alignment\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create sample data with explicit index matching count files\n",
    "        sample_data = pd.DataFrame({\n",
    "            'sample': sample_info['sample'],\n",
    "            'condition': sample_info['condition']\n",
    "        }).reset_index(drop=True)\n",
    "        \n",
    "        # Ensure count files match sample order\n",
    "        print(\"\\nVerifying alignment:\")\n",
    "        print(\"Sample data:\")\n",
    "        print(sample_data)\n",
    "        print(\"\\nCount files:\")\n",
    "        for cf in count_files:\n",
    "            print(os.path.basename(cf))\n",
    "            \n",
    "        # Sort count files to match sample order\n",
    "        sorted_count_files = []\n",
    "        for sample in sample_data['sample']:\n",
    "            matching_file = [f for f in count_files if sample in os.path.basename(f)]\n",
    "            if matching_file:\n",
    "                sorted_count_files.append(matching_file[0])\n",
    "            else:\n",
    "                raise ValueError(f\"No matching count file found for sample {sample}\")\n",
    "        \n",
    "        print(\"\\nAligned count files:\")\n",
    "        for sample, cf in zip(sample_data['sample'], sorted_count_files):\n",
    "            print(f\"{sample}: {os.path.basename(cf)}\")\n",
    "        \n",
    "        # Convert to R objects\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "            sample_data_r = ro.conversion.py2rpy(sample_data)\n",
    "        \n",
    "        # Create DEXSeqDataSet with verbose error checking\n",
    "        print(\"\\nCreating DEXSeqDataSet with:\")\n",
    "        print(f\"Number of samples: {len(sample_data)}\")\n",
    "        print(f\"Number of count files: {len(sorted_count_files)}\")\n",
    "        \n",
    "        dxd = dexseq.DEXSeqDataSetFromHTSeq(\n",
    "            countfiles=ro.StrVector(sorted_count_files),\n",
    "            sampleData=sample_data_r,\n",
    "            design=Formula('~ sample + exon + condition:exon'),\n",
    "            flattenedfile=\"DATA/gencode.v31.basic.annotation.DEXSeq.gff\"\n",
    "        )\n",
    "        \n",
    "        return dxd\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DEXSeqDataSet: {str(e)}\")\n",
    "        # Additional error information\n",
    "        print(\"\\nDetailed error check:\")\n",
    "        print(f\"Sample data shape: {sample_data.shape}\")\n",
    "        print(\"Sample data columns:\", sample_data.columns.tolist())\n",
    "        print(\"\\nCount file contents verification:\")\n",
    "        for cf in sorted_count_files:\n",
    "            print(f\"\\nChecking {os.path.basename(cf)}\")\n",
    "            try:\n",
    "                with open(cf, 'r') as f:\n",
    "                    first_lines = [next(f) for _ in range(5)]\n",
    "                print(\"First 5 lines:\")\n",
    "                for line in first_lines:\n",
    "                    print(line.strip())\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file: {e}\")\n",
    "        raise\n",
    "\n",
    "def verify_sample_data(sample_info, count_files):\n",
    "    \"\"\"\n",
    "    Verify sample data matches count files exactly\n",
    "    \"\"\"\n",
    "    print(\"\\nVerifying sample data and count files alignment:\")\n",
    "    \n",
    "    # Check sample counts\n",
    "    print(f\"Number of samples in sample_info: {len(sample_info)}\")\n",
    "    print(f\"Number of count files: {len(count_files)}\")\n",
    "    \n",
    "    # Create mapping of samples to files\n",
    "    sample_file_map = {}\n",
    "    for cf in count_files:\n",
    "        basename = os.path.basename(cf)\n",
    "        for sample in sample_info['sample']:\n",
    "            if sample in basename:\n",
    "                sample_file_map[sample] = cf\n",
    "                break\n",
    "    \n",
    "    # Check for missing mappings\n",
    "    missing_samples = set(sample_info['sample']) - set(sample_file_map.keys())\n",
    "    extra_files = [cf for cf in count_files if not any(s in os.path.basename(cf) for s in sample_info['sample'])]\n",
    "    \n",
    "    if missing_samples:\n",
    "        print(\"\\nWARNING: Samples without matching files:\")\n",
    "        for sample in missing_samples:\n",
    "            print(f\"  - {sample}\")\n",
    "    \n",
    "    if extra_files:\n",
    "        print(\"\\nWARNING: Count files without matching samples:\")\n",
    "        for ef in extra_files:\n",
    "            print(f\"  - {os.path.basename(ef)}\")\n",
    "    \n",
    "    # Print mapping\n",
    "    print(\"\\nSample to file mapping:\")\n",
    "    for sample in sample_info['sample']:\n",
    "        matched_file = sample_file_map.get(sample, \"NO MATCH\")\n",
    "        if isinstance(matched_file, str):\n",
    "            matched_file = os.path.basename(matched_file)\n",
    "        print(f\"  {sample} -> {matched_file}\")\n",
    "    \n",
    "    return len(missing_samples) == 0 and len(extra_files) == 0\n",
    "\n",
    "# Update main processing code\n",
    "if len(processed_files) == len(edo_nd1_samples):\n",
    "    print(\"\\nAll files processed successfully\")\n",
    "    \n",
    "    # Verify sample data alignment\n",
    "    if verify_sample_data(edo_nd1_samples, processed_files):\n",
    "        print(\"\\nSample data and count files aligned correctly\")\n",
    "        edo_nd1_samples['count_file'] = processed_files\n",
    "        \n",
    "        try:\n",
    "            # Create the DEXSeq dataset\n",
    "            dxd = create_dexseq_dataset(edo_nd1_samples, processed_files, dexseq)\n",
    "            print(\"Successfully created DEXSeq dataset!\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError creating DEXSeq dataset: {str(e)}\")\n",
    "            # Print sample data for debugging\n",
    "            print(\"\\nSample data:\")\n",
    "            print(edo_nd1_samples)\n",
    "            print(\"\\nProcessed files:\")\n",
    "            for pf in processed_files:\n",
    "                print(os.path.basename(pf))\n",
    "    else:\n",
    "        print(\"\\nSample data and count files misaligned\")\n",
    "else:\n",
    "    print(f\"\\nWarning: Only processed {len(processed_files)} out of {len(edo_nd1_samples)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few lines of an original count file\n",
    "with open(count_files[0], 'r') as f:\n",
    "    print(\"First 10 lines of original count file:\")\n",
    "    print(f.read(500))  # First 500 characters\n",
    "\n",
    "# Check the data types of the values\n",
    "df = pd.read_csv(count_files[0], sep='\\s+', header=None)\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nSample of unique values in first column:\")\n",
    "print(df[0].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_count_file_format(file_path):\n",
    "    \"\"\"\n",
    "    Check the format of a count file in detail\n",
    "    \"\"\"\n",
    "    print(f\"\\nChecking file: {file_path}\")\n",
    "    \n",
    "    # Read first few lines\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()[:5]\n",
    "    \n",
    "    print(\"\\nFirst 5 lines:\")\n",
    "    for i, line in enumerate(lines, 1):\n",
    "        parts = line.strip().split('\\t')\n",
    "        print(f\"\\nLine {i}:\")\n",
    "        print(f\"Number of columns: {len(parts)}\")\n",
    "        print(f\"Content: {parts}\")\n",
    "        \n",
    "        # Check format of feature ID\n",
    "        feature_id = parts[0]\n",
    "        if ':' in feature_id:\n",
    "            gene_id, exon_num = feature_id.split(':')\n",
    "            print(f\"Gene ID: {gene_id}\")\n",
    "            print(f\"Exon number: {exon_num}\")\n",
    "        else:\n",
    "            print(f\"Feature ID format: {feature_id}\")\n",
    "\n",
    "# Check the first processed file\n",
    "check_count_file_format(processed_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edo_nd1_samples)\n",
    "print(count_files)\n",
    "print(\"\\n\")\n",
    "print(dexseq)\n",
    "\n",
    "\n",
    "print(len(edo_nd1_samples))\n",
    "print(len(processed_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dxd = create_dexseq_dataset(edo_nd1_samples, count_files, dexseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_count_file(input_file, output_dir):\n",
    "    \"\"\"\n",
    "    Format count file for DEXSeq with proper space handling\n",
    "    \"\"\"\n",
    "    output_file = os.path.join(output_dir, os.path.basename(input_file).replace('.dexeq_counts', '.formatted.counts'))\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\nProcessing file: {os.path.basename(input_file)}\")\n",
    "        \n",
    "        # Read file using tab separator\n",
    "        df = pd.read_csv(input_file, sep='\\t', header=None, \n",
    "                         names=['feature_id', 'count'],\n",
    "                         quoting=3)  # Turn off quoting\n",
    "        \n",
    "        print(f\"Read {len(df)} lines from file\")\n",
    "        print(\"\\nFirst few lines of input:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Process feature IDs\n",
    "        processed_data = []\n",
    "        for _, row in df.iterrows():\n",
    "            feature_id = row['feature_id'].strip('\"')  # Remove outer quotes\n",
    "            count = row['count']\n",
    "            \n",
    "            # Handle special entries\n",
    "            if feature_id.startswith('_'):\n",
    "                processed_data.append([feature_id, '0', str(int(count))])\n",
    "            else:\n",
    "                try:\n",
    "                    # Split feature ID into components\n",
    "                    gene_id, exon_num = feature_id.split('\":\"')\n",
    "                    exon_num = exon_num.strip('\"')\n",
    "                    \n",
    "                    # Format line for DEXSeq\n",
    "                    processed_data.append([gene_id, exon_num, str(int(count))])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing line {feature_id}: {str(e)}\")\n",
    "                    continue\n",
    "        \n",
    "        if not processed_data:\n",
    "            raise ValueError(\"No valid data was processed\")\n",
    "            \n",
    "        # Create and save formatted DataFrame\n",
    "        formatted_df = pd.DataFrame(processed_data, \n",
    "                                    columns=['gene_id', 'exon_id', 'count'])\n",
    "        \n",
    "        # Save without index and headers, tab-separated\n",
    "        formatted_df.to_csv(output_file, sep='\\t', index=False, header=False)\n",
    "        \n",
    "        print(f\"\\nProcessed file: {os.path.basename(output_file)}\")\n",
    "        print(\"First few lines of output:\")\n",
    "        print(formatted_df.head().to_string(index=False))\n",
    "        \n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting file {input_file}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def prepare_sample_info(sample_info):\n",
    "    \"\"\"\n",
    "    Prepare sample information DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter for EDO and ND1 samples\n",
    "        samples = sample_info[sample_info['condition'].isin(['EDO', 'ND1'])].copy()\n",
    "        \n",
    "        # Reset index\n",
    "        samples = samples.reset_index(drop=True)\n",
    "        \n",
    "        # Convert data types\n",
    "        samples['sample'] = samples['sample'].astype(str)\n",
    "        samples['condition'] = pd.Categorical(samples['condition'])\n",
    "        samples['replicate'] = samples['replicate'].astype(str)\n",
    "        \n",
    "        # Set sample as index\n",
    "        samples.set_index('sample', inplace=True)\n",
    "        \n",
    "        return samples\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing sample info: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_dexseq_dataset(sample_info, formatted_files, dexseq):\n",
    "    \"\"\"\n",
    "    Create DEXSeqDataSet with proper sample ordering and include 'sample' as a column.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Reset index to have 'sample' as a column\n",
    "        sample_data = sample_info.reset_index()\n",
    "\n",
    "        # Ensure 'sample' and 'condition' are present\n",
    "        sample_data = sample_data[['sample', 'condition']]\n",
    "\n",
    "        # Convert 'sample' and 'condition' to string (if not already)\n",
    "        sample_data['sample'] = sample_data['sample'].astype(str)\n",
    "        sample_data['condition'] = sample_data['condition'].astype(str)\n",
    "\n",
    "        # Create a mapping of sample names to their corresponding count files\n",
    "        sample_to_file = {}\n",
    "        for file_path in formatted_files:\n",
    "            sample_name = os.path.splitext(os.path.basename(file_path))[0].replace('.formatted', '')\n",
    "            sample_to_file[sample_name] = file_path\n",
    "\n",
    "        # Order the count files to match the sample_data order\n",
    "        ordered_files = []\n",
    "        for sample in sample_data['sample']:\n",
    "            if sample not in sample_to_file:\n",
    "                raise ValueError(f\"Sample {sample} not found in count files\")\n",
    "            ordered_files.append(sample_to_file[sample])\n",
    "\n",
    "        print(\"\\nSample data for DEXSeq:\")\n",
    "        print(sample_data)\n",
    "\n",
    "        print(\"\\nOrdered count files:\")\n",
    "        for sample, file in zip(sample_data['sample'], ordered_files):\n",
    "            print(f\"{sample}: {os.path.basename(file)}\")\n",
    "\n",
    "        # Convert to R\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "            sample_data_r = ro.conversion.py2rpy(sample_data)\n",
    "\n",
    "        # Create count files vector with correct ordering\n",
    "        count_files_vector = ro.StrVector(ordered_files)\n",
    "\n",
    "        # Create DEXSeqDataSet\n",
    "        dxd = dexseq.DEXSeqDataSetFromHTSeq(\n",
    "            countfiles=count_files_vector,\n",
    "            sampleData=sample_data_r,\n",
    "            design=Formula('~ sample + exon + condition:exon'),\n",
    "            flattenedfile=\"/beegfs/scratch/ric.broccoli/ric.broccoli/PW_RNA_seq_deep/gencode.v31.basic.annotation.DEXSeq.gff\"\n",
    "        )\n",
    "\n",
    "        return dxd\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating DEXSeqDataSet: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "temp_dir = \"temp_dexseq_counts\"\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "# Format count files\n",
    "print(\"\\nFormatting count files...\")\n",
    "formatted_files = []\n",
    "for _, row in edo_nd1_samples.iterrows():\n",
    "    formatted_file = format_count_file(row['count_file'], temp_dir)\n",
    "    if formatted_file:\n",
    "        formatted_files.append(formatted_file)\n",
    "\n",
    "if not formatted_files:\n",
    "    raise ValueError(\"No files were formatted successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sample names from formatted files\n",
    "sample_names = [os.path.splitext(os.path.basename(file))[0] for file in formatted_files]\n",
    "\n",
    "# Prepare sample information\n",
    "print(\"\\nPreparing sample information...\")\n",
    "# Reset index to include 'sample' as a column\n",
    "sample_info = edo_nd1_samples.reset_index()\n",
    "\n",
    "# Update 'count_file' paths to the formatted files\n",
    "sample_info['count_file'] = formatted_files\n",
    "\n",
    "# Update 'sample' column with the correct names\n",
    "sample_info['sample'] = sample_names\n",
    "\n",
    "# Ensure proper data types\n",
    "sample_info['sample'] = sample_info['sample'].astype(str)\n",
    "sample_info['condition'] = sample_info['condition'].astype(str)\n",
    "sample_info['replicate'] = sample_info['replicate'].astype(str)\n",
    "\n",
    "# Now, sample_info should have 'sample' as a column with the correct sample names\n",
    "print(\"\\nUpdated sample_info DataFrame:\")\n",
    "print(sample_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DEXSeqDataSet\n",
    "print(\"\\nCreating DEXSeqDataSet...\")\n",
    "dexseq = importr('DEXSeq')\n",
    "dxd = create_dexseq_dataset(sample_info, formatted_files, dexseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import R packages and create DEXSeqDataSet\n",
    "print(\"\\nCreating DEXSeqDataSet...\")\n",
    "dexseq = importr('DEXSeq')\n",
    "dxd = create_dexseq_dataset(sample_info, formatted_files, dexseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(temp_dir):\n",
    "    shutil.rmtree(temp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DEXSeqDataSet with the filtered samples\n",
    "dxd = dexseq.DEXSeqDataSetFromHTSeq(\n",
    "    countfiles=ro.StrVector(edo_nd1_samples['count_file'].tolist()),\n",
    "    sampleData=edo_nd1_samples,\n",
    "    design=r('~ sample + exon + condition:exon'),\n",
    "    flattenedfile=os.path.join(working_dir, \"gencode.v31.basic.annotation.gff\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2.robjects import pandas2ri, r\n",
    "import rpy2.robjects as ro\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Enable automatic conversion between pandas and R dataframes\n",
    "pandas2ri.activate()\n",
    "\n",
    "def format_count_file_for_dexseq(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Reformat count file to match DEXSeq expectations:\n",
    "    From: \"ENSG00000000003.14\":\"001\"    107\n",
    "    To:   ENSG00000000003.14    001    107\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the original file\n",
    "        df = pd.read_csv(input_file, sep='\\t', header=None, \n",
    "                        names=['feature_id', 'count'])\n",
    "        \n",
    "        # Split the feature_id into gene_id and exon_number\n",
    "        df['gene_id'] = df['feature_id'].apply(lambda x: x.split(':')[0].strip('\"'))\n",
    "        df['exon_number'] = df['feature_id'].apply(lambda x: x.split(':')[1].strip('\"') if ':' in x else None)\n",
    "        \n",
    "        # Write in DEXSeq expected format\n",
    "        with open(output_file, 'w') as f:\n",
    "            for _, row in df.iterrows():\n",
    "                if row['exon_number'] is not None:\n",
    "                    f.write(f\"{row['gene_id']}\\t{row['exon_number']}\\t{row['count']}\\n\")\n",
    "                else:\n",
    "                    # Handle special cases like _ambiguous, _empty, etc.\n",
    "                    f.write(f\"{row['feature_id']}\\t0\\t{row['count']}\\n\")\n",
    "                    \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting file {input_file}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def prepare_sample_info(sample_info_df):\n",
    "    \"\"\"\n",
    "    Prepare sample information and reformat count files.\n",
    "    \"\"\"\n",
    "    # Filter for EDO and ND1 samples\n",
    "    edo_nd1_samples = sample_info_df[sample_info_df['condition'].isin(['EDO', 'ND1'])].copy()\n",
    "    edo_nd1_samples = edo_nd1_samples.reset_index(drop=True)\n",
    "    \n",
    "    # Create reformatted count files\n",
    "    reformatted_files = []\n",
    "    for idx, row in edo_nd1_samples.iterrows():\n",
    "        original_file = row['count_file']\n",
    "        reformatted_file = original_file + '.reformatted'\n",
    "        if format_count_file_for_dexseq(original_file, reformatted_file):\n",
    "            reformatted_files.append(reformatted_file)\n",
    "        else:\n",
    "            raise Exception(f\"Failed to format count file: {original_file}\")\n",
    "    \n",
    "    # Update count_file paths\n",
    "    edo_nd1_samples['count_file'] = reformatted_files\n",
    "    \n",
    "    # Ensure proper categorical variables\n",
    "    edo_nd1_samples['condition'] = pd.Categorical(edo_nd1_samples['condition'])\n",
    "    edo_nd1_samples['replicate'] = pd.Categorical(edo_nd1_samples['replicate'])\n",
    "    \n",
    "    return edo_nd1_samples\n",
    "\n",
    "def run_dexseq_analysis(sample_info_df, working_dir, padj_threshold=0.05, log2fc_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Run DEXSeq analysis with reformatted files.\n",
    "    \"\"\"\n",
    "    print(\"Sample groups in data:\")\n",
    "    print(sample_info_df['condition'].value_counts())\n",
    "    \n",
    "    # Prepare samples and reformat files\n",
    "    print(\"\\nPreparing count files...\")\n",
    "    edo_nd1_samples = prepare_sample_info(sample_info_df)\n",
    "    print(\"\\nFiltered samples:\")\n",
    "    print(edo_nd1_samples)\n",
    "    \n",
    "    try:\n",
    "        # Import R packages\n",
    "        dexseq = importr('DEXSeq')\n",
    "        deseq2 = importr('DESeq2')\n",
    "        \n",
    "        # Create DEXSeqDataSet\n",
    "        print(\"\\nCreating DEXSeqDataSet...\")\n",
    "        dxd = dexseq.DEXSeqDataSetFromHTSeq(\n",
    "            countfiles=ro.StrVector(edo_nd1_samples['count_file'].tolist()),\n",
    "            sampleData=edo_nd1_samples,\n",
    "            design=r('~ sample + exon + condition:exon'),\n",
    "            flattenedfile=os.path.join(working_dir, \"gencode.v31.basic.annotation.gff\")\n",
    "        )\n",
    "        \n",
    "        # Continue with analysis as before...\n",
    "        print(\"Estimating size factors...\")\n",
    "        dxd = dexseq.estimateSizeFactors(dxd)\n",
    "        \n",
    "        print(\"Estimating dispersions...\")\n",
    "        r.pdf(\"dispersion_estimates.pdf\")\n",
    "        dxd = dexseq.estimateDispersions(dxd, quiet=False)\n",
    "        r('dev.off()')\n",
    "        \n",
    "        print(\"Testing for differential exon usage...\")\n",
    "        dxd = dexseq.testForDEU(dxd)\n",
    "        dxd = dexseq.estimateExonFoldChanges(dxd, fitExpToVar=\"condition\")\n",
    "        \n",
    "        # Get results\n",
    "        print(\"Processing results...\")\n",
    "        res = dexseq.DEXSeqResults(dxd)\n",
    "        res_df = pandas2ri.rpy2py_dataframe(res)\n",
    "        \n",
    "        # Process results\n",
    "        return process_results(res_df, padj_threshold, log2fc_threshold)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in DEXSeq analysis: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Clean up reformatted files\n",
    "        for file in edo_nd1_samples['count_file']:\n",
    "            if os.path.exists(file):\n",
    "                os.remove(file)\n",
    "\n",
    "def process_results(res_df, padj_threshold, log2fc_threshold):\n",
    "    \"\"\"\n",
    "    Process DEXSeq results with corrected statistics.\n",
    "    \"\"\"\n",
    "    # Add detailed statistics\n",
    "    results_with_stats = res_df.copy()\n",
    "    results_with_stats['log2FoldChange'] = np.log2(res_df['exonBaseMean'].replace(0, np.nan))\n",
    "    \n",
    "    # Handle potential infinity values\n",
    "    results_with_stats['log2FoldChange'] = results_with_stats['log2FoldChange'].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Calculate percentage usage\n",
    "    results_with_stats['percentageUsage'] = (res_df['countData'] / \n",
    "                                           res_df['exonBaseMean'].replace(0, np.nan) * 100)\n",
    "\n",
    "    # Filter significant results\n",
    "    significant_results = results_with_stats[\n",
    "        (results_with_stats['padj'] < padj_threshold) & \n",
    "        (abs(results_with_stats['log2FoldChange']) > log2fc_threshold)\n",
    "    ].copy()\n",
    "\n",
    "    # Generate summary statistics\n",
    "    summary_stats = create_summary_statistics(results_with_stats, significant_results)\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_visualization_plots(results_with_stats, significant_results, \n",
    "                             padj_threshold, log2fc_threshold)\n",
    "\n",
    "    # Save results\n",
    "    save_results(results_with_stats, significant_results, summary_stats)\n",
    "\n",
    "    return results_with_stats, significant_results, summary_stats\n",
    "\n",
    "def create_summary_statistics(results_df, significant_df):\n",
    "    \"\"\"\n",
    "    Create summary statistics with proper handling of NaN values.\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'Metric': [\n",
    "            'Total Exons Tested',\n",
    "            'Significant Exons (padj < 0.05)',\n",
    "            'Mean Log2 Fold Change (significant)',\n",
    "            'Median adjusted p-value (significant)',\n",
    "            'Number of Upregulated Exons',\n",
    "            'Number of Downregulated Exons'\n",
    "        ],\n",
    "        'Value': [\n",
    "            len(results_df),\n",
    "            len(significant_df),\n",
    "            significant_df['log2FoldChange'].mean(),\n",
    "            significant_df['padj'].median(),\n",
    "            sum(significant_df['log2FoldChange'] > 0),\n",
    "            sum(significant_df['log2FoldChange'] < 0)\n",
    "        ]\n",
    "    })\n",
    "\n",
    "def create_volcano_plot(results_df, padj_threshold, log2fc_threshold):\n",
    "    plt.scatter(results_df['log2FoldChange'], -np.log10(results_df['padj']), alpha=0.5)\n",
    "    plt.axhline(-np.log10(padj_threshold), color='red', linestyle='--')\n",
    "    plt.axvline(-log2fc_threshold, color='red', linestyle='--')\n",
    "    plt.axvline(log2fc_threshold, color='red', linestyle='--')\n",
    "    plt.xlabel('Log2 Fold Change')\n",
    "    plt.ylabel('-Log10 Adjusted p-value')\n",
    "    plt.title('Volcano Plot')\n",
    "\n",
    "def create_ma_plot(results_df):\n",
    "    plt.scatter(np.log10(results_df['baseMean']), results_df['log2FoldChange'], alpha=0.5)\n",
    "    plt.xlabel('Log10 Mean Expression')\n",
    "    plt.ylabel('Log2 Fold Change')\n",
    "    plt.title('MA Plot')\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    \n",
    "def create_visualization_plots(results_df, significant_df, padj_threshold, log2fc_threshold):\n",
    "    \"\"\"\n",
    "    Create visualization plots with proper data handling.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Volcano plot\n",
    "    plt.subplot(2, 2, 1)\n",
    "    create_volcano_plot(results_df, padj_threshold, log2fc_threshold)\n",
    "\n",
    "    # MA plot\n",
    "    plt.subplot(2, 2, 2)\n",
    "    create_ma_plot(results_df)\n",
    "\n",
    "    # Save plots\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dexseq_analysis_plots.pdf')\n",
    "    plt.close()\n",
    "\n",
    "def save_results(results_df, significant_df, summary_stats):\n",
    "    \"\"\"\n",
    "    Save analysis results to files.\n",
    "    \"\"\"\n",
    "    results_df.to_csv(\"all_dexseq_results.csv\", index=False)\n",
    "    significant_df.to_csv(\"significant_dexseq_results.csv\", index=False)\n",
    "    summary_stats.to_csv(\"analysis_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_file_format(file_path):\n",
    "    \"\"\"\n",
    "    Check if the file format is correct and print diagnostic information.\n",
    "    \"\"\"\n",
    "    print(f\"\\nChecking file: {file_path}\")\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            first_lines = [next(f) for _ in range(5)]\n",
    "        print(\"First 5 lines:\")\n",
    "        for line in first_lines:\n",
    "            print(line.strip())\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def format_count_file_for_dexseq(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Reformat count file to match DEXSeq expectations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\nFormatting file: {input_file}\")\n",
    "        # Read the original file\n",
    "        df = pd.read_csv(input_file, sep='\\t', header=None, \n",
    "                        names=['feature_id', 'count'], quoting=3)\n",
    "        \n",
    "        # Split the feature_id into gene_id and exon_number\n",
    "        def parse_feature_id(feature_id):\n",
    "            if feature_id.startswith('*') or feature_id.startswith('_'):\n",
    "                return feature_id, 'NA'\n",
    "            try:\n",
    "                parts = feature_id.replace('\"', '').split(':')\n",
    "                gene_id = parts[0]\n",
    "                exon_number = parts[1] if len(parts) > 1 else 'NA'\n",
    "                return gene_id, exon_number\n",
    "            except:\n",
    "                return feature_id, 'NA'\n",
    "        \n",
    "        # Apply parsing\n",
    "        df[['gene_id', 'exon_number']] = df.apply(\n",
    "            lambda x: pd.Series(parse_feature_id(x['feature_id'])), axis=1)\n",
    "        \n",
    "        # Write in DEXSeq expected format\n",
    "        df.to_csv(output_file, sep='\\t', \n",
    "                 columns=['gene_id', 'exon_number', 'count'],\n",
    "                 header=False, index=False)\n",
    "        \n",
    "        # Verify the output\n",
    "        print(\"\\nVerifying output file format:\")\n",
    "        check_file_format(output_file)\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error formatting file {input_file}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def prepare_sample_info(sample_info_df):\n",
    "    \"\"\"\n",
    "    Prepare sample information and print diagnostic information.\n",
    "    \"\"\"\n",
    "    print(\"\\nPreparing sample information:\")\n",
    "    print(\"Original sample info:\")\n",
    "    print(sample_info_df)\n",
    "    \n",
    "    # Filter for EDO and ND1 samples\n",
    "    edo_nd1_samples = sample_info_df[sample_info_df['condition'].isin(['EDO', 'ND1'])].copy()\n",
    "    edo_nd1_samples = edo_nd1_samples.reset_index(drop=True)\n",
    "    \n",
    "    # Sort by condition and replicate\n",
    "    edo_nd1_samples = edo_nd1_samples.sort_values(['condition', 'replicate'])\n",
    "    \n",
    "    print(\"\\nFiltered and sorted sample info:\")\n",
    "    print(edo_nd1_samples)\n",
    "    \n",
    "    # Create reformatted count files\n",
    "    reformatted_files = []\n",
    "    for idx, row in edo_nd1_samples.iterrows():\n",
    "        original_file = row['count_file']\n",
    "        reformatted_file = original_file + '.reformatted'\n",
    "        if format_count_file_for_dexseq(original_file, reformatted_file):\n",
    "            reformatted_files.append(reformatted_file)\n",
    "        else:\n",
    "            raise Exception(f\"Failed to format count file: {original_file}\")\n",
    "    \n",
    "    # Update count_file paths\n",
    "    edo_nd1_samples['count_file'] = reformatted_files\n",
    "    \n",
    "    # Convert to categorical\n",
    "    edo_nd1_samples['condition'] = pd.Categorical(edo_nd1_samples['condition'])\n",
    "    edo_nd1_samples['replicate'] = pd.Categorical(edo_nd1_samples['replicate'])\n",
    "    \n",
    "    print(\"\\nFinal sample information:\")\n",
    "    print(edo_nd1_samples)\n",
    "    \n",
    "    # Verify all files exist\n",
    "    print(\"\\nVerifying count files:\")\n",
    "    for file in edo_nd1_samples['count_file']:\n",
    "        print(f\"{file}: {os.path.exists(file)}\")\n",
    "    \n",
    "    return edo_nd1_samples\n",
    "\n",
    "def run_dexseq_analysis(sample_info_df, working_dir, padj_threshold=0.05, log2fc_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Run DEXSeq analysis with additional error checking.\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting DEXSeq analysis...\")\n",
    "    print(f\"Working directory: {working_dir}\")\n",
    "    \n",
    "    # Verify GFF file\n",
    "    gff_file = os.path.join(working_dir, \"gencode.v31.basic.annotation.gff\")\n",
    "    if not os.path.exists(gff_file):\n",
    "        raise FileNotFoundError(f\"GFF file not found: {gff_file}\")\n",
    "    print(f\"GFF file exists: {gff_file}\")\n",
    "    \n",
    "    # Prepare samples\n",
    "    try:\n",
    "        edo_nd1_samples = prepare_sample_info(sample_info_df)\n",
    "        \n",
    "        # Import R packages\n",
    "        print(\"\\nImporting R packages...\")\n",
    "        dexseq = importr('DEXSeq')\n",
    "        deseq2 = importr('DESeq2')\n",
    "        \n",
    "        # Convert sample information to R\n",
    "        print(\"\\nConverting sample information to R format...\")\n",
    "        r_sample_data = pandas2ri.py2rpy(edo_nd1_samples)\n",
    "        \n",
    "        # Create DEXSeqDataSet\n",
    "        print(\"\\nCreating DEXSeqDataSet...\")\n",
    "        print(\"Count files:\")\n",
    "        for file in edo_nd1_samples['count_file']:\n",
    "            print(f\"- {file}\")\n",
    "            \n",
    "        dxd = dexseq.DEXSeqDataSetFromHTSeq(\n",
    "            countfiles=ro.StrVector(edo_nd1_samples['count_file'].tolist()),\n",
    "            sampleData=r_sample_data,\n",
    "            design=r('~ sample + exon + condition:exon'),\n",
    "            flattenedfile=gff_file\n",
    "        )\n",
    "        \n",
    "        # Continue with analysis...\n",
    "        print(\"Running DEXSeq pipeline...\")\n",
    "        \n",
    "        return run_dexseq_pipeline(dxd, dexseq, padj_threshold, log2fc_threshold)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in DEXSeq analysis: {str(e)}\")\n",
    "        raise\n",
    "        \n",
    "def run_dexseq_pipeline(dxd, dexseq, padj_threshold, log2fc_threshold):\n",
    "    \"\"\"\n",
    "    Run the DEXSeq pipeline steps with error checking.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Estimating size factors...\")\n",
    "        dxd = dexseq.estimateSizeFactors(dxd)\n",
    "        \n",
    "        print(\"Estimating dispersions...\")\n",
    "        r.pdf(\"dispersion_estimates.pdf\")\n",
    "        dxd = dexseq.estimateDispersions(dxd, quiet=False)\n",
    "        r('dev.off()')\n",
    "        \n",
    "        print(\"Testing for differential exon usage...\")\n",
    "        dxd = dexseq.testForDEU(dxd)\n",
    "        \n",
    "        print(\"Estimating exon fold changes...\")\n",
    "        dxd = dexseq.estimateExonFoldChanges(dxd, fitExpToVar=\"condition\")\n",
    "        \n",
    "        print(\"Getting results...\")\n",
    "        res = dexseq.DEXSeqResults(dxd)\n",
    "        res_df = pandas2ri.rpy2py_dataframe(res)\n",
    "        \n",
    "        return process_results(res_df, padj_threshold, log2fc_threshold)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in DEXSeq pipeline: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, print current state\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nSample information:\")\n",
    "print(sample_info)\n",
    "\n",
    "try:\n",
    "    results, significant_results, summary = run_dexseq_analysis(\n",
    "        sample_info, \n",
    "        working_dir=\"/beegfs/scratch/ric.broccoli/kubacki.michal/PW_RNA_seq_deep\",\n",
    "        padj_threshold=0.05,\n",
    "        log2fc_threshold=1.0\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError in analysis: {str(e)}\")\n",
    "    print(\"\\nFull error information:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DEXSeqDataSet with the filtered samples\n",
    "dxd = dexseq.DEXSeqDataSetFromHTSeq(\n",
    "    countfiles=ro.StrVector(edo_nd1_samples['count_file'].tolist()),\n",
    "    sampleData=edo_nd1_samples,\n",
    "    design=r('~ sample + exon + condition:exon'),\n",
    "    flattenedfile=os.path.join(working_dir, \"gencode.v31.basic.annotation.gff\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None:\n",
    "    # Log count distribution\n",
    "    plt.subplot(132)\n",
    "    plt.hist(np.log10(df['count'][df['count'] > 0]), bins=50)\n",
    "    plt.title('Log10 Count Distribution')\n",
    "    plt.xlabel('Log10(Counts)')\n",
    "    plt.ylabel('Frequency')\n",
    "    \n",
    "    # Box plot\n",
    "    plt.subplot(133)\n",
    "    df_filtered = df[~df['feature_id'].str.startswith('*')]\n",
    "    plt.boxplot(df_filtered['count'])\n",
    "    plt.title('Count Boxplot\\n(excluding special features)')\n",
    "    plt.ylabel('Counts')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('count_distributions.pdf')\n",
    "    plt.close()\n",
    "    \n",
    "    # Examine genes of interest\n",
    "    genes_of_interest = [\"SETD5\", \"NSD2\", \"POLE\", \"HTT\", \"PER3\", \"MED14\"]\n",
    "    examine_genes_of_interest(df, genes_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plot count distributions\n",
    "print(\"\\nPlotting count distributions...\")\n",
    "plot_count_distributions(count_files[:3])  # Examine first 3 files\n",
    "plt.savefig('count_distributions.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nChecking for issues in files:\")\n",
    "for file_path in count_files:\n",
    "    issues = check_file_issues(file_path)\n",
    "    if issues:\n",
    "        print(f\"\\n{os.path.basename(file_path)}:\")\n",
    "        for issue in issues:\n",
    "            print(f\"- {issue}\")\n",
    "\n",
    "# Save summary to file\n",
    "with open('dexseq_counts_summary.txt', 'w') as f:\n",
    "    f.write(\"DEXSeq Count Files Summary\\n\")\n",
    "    f.write(\"=========================\\n\\n\")\n",
    "    f.write(\"File Structure Comparison:\\n\")\n",
    "    f.write(structure_comparison.to_string())\n",
    "    f.write(\"\\n\\nSample Statistics:\\n\")\n",
    "    for file_path in count_files:\n",
    "        df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "        f.write(f\"\\n{os.path.basename(file_path)}:\\n\")\n",
    "        f.write(df[2].describe().to_string())\n",
    "\n",
    "print(\"\\nAnalysis complete. Check 'dexseq_counts_summary.txt' for detailed summary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure proper formatting for DEXSeq\n",
    "edo_nd1_samples = edo_nd1_samples.reset_index(drop=True)\n",
    "edo_nd1_samples['condition'] = pd.Categorical(edo_nd1_samples['condition'])\n",
    "edo_nd1_samples['replicate'] = pd.Categorical(edo_nd1_samples['replicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DEXSeqDataSet with the filtered samples\n",
    "dxd = dexseq.DEXSeqDataSetFromHTSeq(\n",
    "    countfiles=ro.StrVector(edo_nd1_samples['count_file'].tolist()),\n",
    "    sampleData=edo_nd1_samples,\n",
    "    design=r('~ sample + exon + condition:exon'),\n",
    "    flattenedfile=os.path.join(working_dir, \"gencode.v31.basic.annotation.gff\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAM FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for RNA-seq data\n",
    "bam_dir = os.path.join(data_dir, \"Bam_file\")\n",
    "bam_files = [f for f in os.listdir(bam_dir) if f.endswith('.bam')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bam_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate patient and control files\n",
    "EDO = [f for f in bam_files if 'EDO' in f]\n",
    "PW1 = [f for f in bam_files if 'PW1' in f]\n",
    "ND1 = [f for f in bam_files if 'ND1' in f]\n",
    "print(EDO)\n",
    "print(PW1)\n",
    "print(ND1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the genes of interest from the pull-down experiment\n",
    "genes_of_interest = [\"SETD5\", \"NSD2\", \"POLE\", \"HTT\", \"PER3\", \"MED14\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_gene_coverage(bam_file, gene_regions):\n",
    "    coverage = {}\n",
    "    with pysam.AlignmentFile(bam_file, \"rb\") as bam:\n",
    "        for gene, region in gene_regions.items():\n",
    "            chrom, start, end = region\n",
    "            coverage[gene] = bam.count_coverage(chrom, start, end)\n",
    "    return coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
